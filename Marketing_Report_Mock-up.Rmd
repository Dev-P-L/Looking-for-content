---
title: "PH125.9x HarvardX - Capstone Examination in Data Science"
subtitle: "Report on Direct Bank Marketing Project"
author: "Philippe Lambot"
date: "April 24, 2019"
output: 
  html_document:
    toc: true
    toc_depth: 4
---
```{r Setup, include = FALSE}

# BEFORE KNITTING THIS FILE PLEASE SEE BELOW "Important Foreword - Requirements". 

# In the first lines of code, in the YAML, I have asked a TOC (table of contents). 

# I have also chosen to produce an html_document and issue it in PDF format, taking into account the numerous graphs and tables. 

# In the next opts_chunk, I'll choose options to avoid messages and warnings in Report.pdf. Messages and warnings produced by the code have already been dealt with while running Scrit.R file.  
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

# The next opts_chunk regulates figure layout.
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")

# The next instruction facilitates table layout in HTML.
options(knitr.table.format = "html")

# After the present chunk, there are 13 lines of code to further regulate layout:
# - the 1st block prevents bullets appearing in the TOC (Table of Contents);
# - the 2nd block determines font size in body text parts;
# - the 3rd block generates text justification.
# Last about layout, I use the string $~$ to generate double empty lines.
```

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0;
}
</style>

<font size="3">

<style>
body {
text-align: justify}
</style>

$~$

$~$

$~$

## $~$

## $~$

## $~$

## *********************************************************************************

## "CHOOSE YOUR OWN" PROJECT: DIRECT BANK MARKETING

## ********************************************************************************* 

## Important Foreword - Requirements

In this project, there are two programs: Script.R and Report.Rmd. 

Script.R has been run in 

 - RStudio Version 1.1.456 - © 2009-2018 RStudio, Inc. 

Report.Rmd has been knitted to HTML in 

 - RStudio Version 1.1.456 - © 2009-2018 RStudio, Inc.

The version of R that I use on my PC is

 - R version 3.5.1 (2018-07-02) -- "Feather Spray"

 - Copyright (C) 2018 The R Foundation for Statistical Computing

 - Platform: x86_64-w64-mingw32/x64 (64-bit)

The operating system on my PC is Windows 10.

I cannot guarantee Script.R running on other versions. Neither can I guarantee Report.Rmd being knitted to HTML on other versions. 

$~$

## I. Introduction

This **predictive and supervised data science project** is about **direct bank marketing** campaigns that a Portuguese bank organized by phone calls among customers to collect term deposits.

This project is presented in the framework of the Capstone Examination in Data Science organized by HarvardX (PH125.9x). It is a "choose your own" project and it is the second step from the Capstone Examination. 

$~$

### A. Dataset

Data has been downloaded from the _UCI Machine Learning Repository_ site, following one suggestion of HavardX's. It is real data.

The dataset is a file with 4521 observations and 17 variables. The 4521 observations relate to 4521 customers whom the bank has contacted by phone calls to propose subscribing a term deposit. Among the 17 variables, there is a variable, y, indicating whether the customer has or has not subscribed a term deposit. From the other 16 variables, 15 will be used as predictors to predict whether the subscriber will or will not subscribe a term deposit.  

$~$

### B. Objective of the Project and Terminology

The objective is as follows: with a model that predicts who will subscribe a term deposit and who will not, reaching, on a validation set, one of the three alternatives from the table hereunder.

$~$

```{r Cleaning up workspace, downloading packages and building objective table.}
# Cleaning up workspace for RAM management.
invisible(if(!is.null(dev.list())) dev.off())
rm(list=ls())
cat("\014")

# Downloading packages.
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# Palliating clash between packages.
# I have already loaded tidyverse, and thus dplyr. Later on I will also load MASS. dplyr and MASS both have a select() function that can mask the other one. To prevent dysfuntioning, I will particularize the select() function from dplyr and will systematically use dselect() instead of select().
dselect <- dplyr::select 

# Building up objective table.
subs_pct <- c(76, 75, 74)
reduction_pct <- c(49, 50, 51)
obj <- c("Alternative 1", "Alternative 2", "Alternative 3")
objective_table <- data.frame(obj, subs_pct, reduction_pct) %>% 
  mutate(subs_pct = paste(">=", subs_pct, "%", sep = " ")) %>%
  mutate(reduction_pct = paste(">=", reduction_pct, "%", sep = " ")) %>%         
  rename(Objective = obj, Subscribers_Reached  = subs_pct,
         Global_Coverage_Reduction = reduction_pct)
kable(objective_table, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16)
rm(subs_pct, reduction_pct, obj)
```

$~$

The percentage of subscribers reached is calculated as follows: the number of customers that are predicted as subscribers by the model and that really are subscribers, divided by the number of subscribers. It is multiplied by 100 to have percentage points instead of decimals. In more technical words, the percentage of subscribers reached is sensitivity (or recall) multiplied by 100. It equals TP / (TP + FN) * 100. 

The global coverage reduction is the percentage of customers that the model considers as non-subscribers. It is called "global coverage reduction" because it indicates by how much the list of customers to be contacted can be reduced while still reaching a specified percentage of subscribers. In more technical terms, the global coverage reduction is equal to (FN + TN) divided by the number of customers (and multiplied by 100). 

Alternative 1 (76-49), alternative 2 (75-50) and alternative 3 (74-51) are perfect substitutes for each other and are considered as perfectly equivalent in terms of reaching the objective. If one alternative is met, then the objective of the project is fully met. For instance 76-49 on the validation set perfectly attains the objective since it meets alternative 1. Other acceptable example: 74-51 also reaches the objective since it meets alternative 3. Third acceptable example: 77-50 also attains the objective; it meets alternative 1 and alternative 2 (it even beats alternatives 1 and 2). 

The objective must be reached on a 30% validation set that can only be used at the very last step with the final model. Of course, the final model cannot use, in predicting, any information coming from the validation set dependent variable, i.e. the variable indicating whether the customer has really subscribed a term deposit or not. The validation set dependent variable can only be used at the very last step to check up whether prediction is right or not and so to measure performance. 

When checking up validity of results in this binary classification challenge, results are rounded to the nearest percentage point. As an example, 49.499% becomes 49%.

Outside the validation set, the remaining 70% of data can be further split, rearranged and analyzed in any way.

By the way, in this project, the words "objective" and "target" are synonyms.

$~$

### C. Key Steps

There are several key steps in the project, as indicated in the table of contents on the first page from Report.pdf. 

After downloading the dataset, it will be prepared and split. A 30% validation will be extracted, as required in the objective of the project. The remaining 70% of data will be further split into an 80% training set and a 20% test set, which will both be used to train models. 

Another section will describe the attributes, i.e. the dependent variable and the predictors. It will be followed by exploratory analysis and visualization. Insights will be gained through this process. One predictor will be discarded because of a fundamental bias. 

Four machine learning models will then be trained on the training set and on the test set. After trying a dimension reduction, performance improvement will be sought through tuning probability threshold below 0.5, with promising results. An ensemble model will be built with a view to more result stability. The whole modeling and analysis process will deliver numerous insights. 

The ensemble model will then be validated on the validation set, producing the final results, which will be evaluated by comparing with the objective.

$~$

### D.	File and Document Organization

In this project, there are four files: Script.R, Report.Rmd, Report.pdf and bank.csv. They can all be accessed in the GitHub repository https://github.com/Dev-P-L/Bank-Marketing . 

Code is included both in Script.R and in Report.Rmd. Code is very similar in both files (the exceptions being the chunk delimiters in Report.Rmd and the code generating the objective table in Report.Rmd). Script.R also contains comments about code. Results generated by both files are the same. 

Report.Rmd describes data analysis, modeling, insights and results. 

Knitting Report.Rmd (to HTML) generates an HTML document that contains data analysis, modeling, insights and results; that HTML document can be easily issued in PDF format as Report.pdf. For readability reasons, Report.pdf does not contain any code. 

Dear Readers, you can run Script.R, knit Report.Rmd (to HTML) and issue it in PDF format. On my laptop, running Script.R takes approximately a quarter of an hour. Knitting Report.Rmd (to HTML) also takes more or less a quarter of an hour. Before running Script.R or knitting Report.Rmd to HTML, please read hereinabove "Important Foreword - Requirements". 

Some packages are required by both Script.R and Report.Rmd: _tidyverse_, _caret_, _kableExtra_, _MASS_, _randomForest_ and _gbm_; the code contains instructions to download them if they are not available.

$~$

## II. Downloading Data

I have downloaded data as follows.

1. Following the link: http://archive.ics.uci.edu/ml/datasets/Bank+Marketing .

2. "Clicking" on "Data Folder" to download "bank.zip".

3. Unzipping "bank.zip".

4. Extracting "bank.csv".

5. Saving "bank.csv" to my GitHub repository.

Now, let's retrieve "bank.csv" from my GitHub repository by using the read.csv() function and accessing https://raw.githubusercontent.com/Dev-P-L/Bank-Marketing/master/bank.csv.

```{r Dwonloading data}
myfile <- "https://raw.githubusercontent.com/Dev-P-L/Bank-Marketing/master/bank.csv"
bank <- read.csv(myfile)
rm(myfile)
```

$~$

## III. Preparing and Splitting Data

Let's first adapt the symbols used as values in the target variable. The target or dependent variable is of binary categorical type _no/yes_. In order to have _yes_ (subscriptions of deposits) as "positive" class in confusion matrices and performance measures (sensitivity and precision), I am going to rename the _no/yes_ values as *no_deposit* and _deposit_. 

```{r Changing symbols in dependent variable for clarity of results.}
bank <- bank %>% mutate(y = as.character(y)) 
bank$y <- gsub("no", "no_deposit", bank$y)
bank$y <- gsub("yes", "deposit", bank$y)
bank <- bank %>% mutate(y = as.factor(y)) %>% as.data.frame()
```

Let's create the required 30% validation set that will only be used at the very last step and only with the final model. 

```{r Creating validation set.}
set.seed(1)
ind <- createDataPartition(y = bank$y, times = 1, p = 0.3, list = FALSE)
temp <- bank[-ind,]
bank_val <- bank[ind,]
rm(ind)
```

Let's further split the data outside the validation set into a training set and a test set. Both will be used to train models.

```{r Splitting temp into training set and test set.}
set.seed(1)
ind <- createDataPartition(y = temp$y, times = 1, p = 0.2, list = FALSE)
bank_train <- temp[-ind,]
bank_test <- temp[ind,]
rm(ind, temp)
```

There are no missing values.

$~$

## IV. Description of Attributes

### A. Description of Target or Dependent Variable 

Let's quickly remember the main characteristics of the target variable, y. 

$~$

```{r Description of target variable}
tab <- data.frame(Variable_Name = "y", 
  Type = "Binary categorical - Response: deposit/no_deposit", 
  Meaning = "Has the customer subscribed a term deposit?") 
kable(tab, "html", align = "c") %>% kable_styling(bootstrap_options = "bordered", 
  full_width = F, font_size = 16) 
rm(tab)
```

$~$

### B. Description of Predictors 

Let's build up a descriptive table of predictors. Which is the name of the predictors in bank.csv? Which is the type of variable? From a marketing point of view, which is the meaning of each predictor? Which type of information does each predictor bring, is it information about customer or about campaign action, is it financial or non financial?

$~$

```{r Description of predictors}
# First, name of variables and type in R
temp <- bank_train %>% dselect(- y)
predictors_tab <- sapply(temp, class) %>% as.data.frame() %>% setNames("type")
predictors_tab <- data.frame(Variable = row.names(predictors_tab), 
                             R_Type = predictors_tab$type)
rm(temp)

# Then, meaning of variables in marketing
Meaning_in_Marketing_Campaign <- c("Age in years", "Professional status",
  "Marital status", "Education", "Credit in default?", "Account balance",
  "Home loan?", "Personal loan?",
  "Medium of communication with this customer in this campaign",
  "Day (of the month) of last contact with this customer in this campaign",
  "Month of last contact with this customer in this campaign",
  "Duration in seconds of last contact with this customer in this campaign",
  "Number of contacts with this customer in this campaign",
  "Number of days since last contact with this customer in previous campaign",
  "Number of contacts with this customer before this campaign",
  "Outcome from previous marketing campaign with this customer")

# Types of information in marketing
Information_Type <- c(seq(1,1, length.out = 4), seq(2,2, length.out = 4),
                      seq(3, 3, length.out = 5), seq(4, 4, length.out = 3))
Information_Type <- gsub(1, "Customer's NON-FINANCIAL Profile", Information_Type)
Information_Type <- gsub(2, "Customer's FINANCIAL Profile", Information_Type)
Information_Type <- gsub(3, "THIS Marketing Campaign", Information_Type)
Information_Type <- gsub(4, "PREVIOUS Marketing Campaign", Information_Type)

# Assembling everything into a descriptive table of predictors.
predictors_tab <- data.frame(predictors_tab, Meaning_in_Marketing_Campaign, 
                             Information_Type)
kable(predictors_tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
  full_width = F, font_size = 16) %>% column_spec(1:2, width = "1in") %>% 
  column_spec(3:4, width = "3.5in") 
rm(Meaning_in_Marketing_Campaign, Information_Type, predictors_tab)
```

$~$

## V. Exploratory Analysis and Visualization

### A. Target or Dependent Variable, y

```{r Exploratory analysis of y}
tab <- summary(bank_train$y)
stats <- tab %>% as.vector()
total = sum(stats)
tab <- data.frame(y_value = names(tab), Occurrences_in_Training_Set = stats, 
                  Percentage = (stats / total) * 100) %>% 
  mutate(Percentage = format(Percentage, digits = 3, nsmall = 1))
kable(tab, "html", align = "c") %>% kable_styling(bootstrap_options = "bordered", 
  full_width = F, font_size = 16) %>% column_spec(1:3, width = "2.5in")
rm(stats, total, tab)
```

$~$

Prevalence of "deposit" is very low, i.e. 11.5%. Consequently, a high accuracy of 0.885 would mean nothing since we would already reach 0.885 simply by predicting "no_deposit" for all customers! Other measurement tools have to be used, ensuring some levels of performance on the subgroup of subscribers, who represent 11.5% of the customers. 

$~$

### B. Age 

Let's build up a histogram of customers according to age. 

$~$

```{r Histogram of age}
graph <- bank_train %>% dselect(age) %>% as.data.frame() %>% 
  ggplot(aes(age)) + 
  geom_histogram(bins = 30, color = "#007ba7", fill = "#9bc4e2") +
  ggtitle("Customers per Age in Training Set") + xlab("Age") + ylab("Count of Customers") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(size = 12), axis.text.y =element_text(size = 12))
graph
rm(graph)
```

$~$

The most populated subgroups are, in descending order, the 30-somethings, the 40-somethings and the 50-somethings. Will the same concentration be found in terms of subscription percentages? Let's have a look at a table where subscription percentages are sorted in descending order. 

$~$

```{r Table about age}
tab <- bank_train %>% dselect(y, age) %>% 
  mutate(age = cut(as.numeric(age), breaks = c(19, 25, 30, 35, 40, 45, 50, 55, 61, 88), 
                   right = FALSE, dig.lab = 5)) %>%
  mutate(y = as.numeric(y) - 1) %>% group_by(age) %>%
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>% 
  mutate(percent_yes = format(percent_yes, digits = 1, nsmall = 0)) %>% 
  arrange(desc(percent_yes)) %>%
  rename(Age = age, Number_of_Customers = n, Subscription_Percentage = percent_yes) %>% 
  as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
  full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(tab)
```

$~$

Actually, age categories that are more populated tend to have lower subscription
percentages (but negative correlation is not causation!). The highest subscription percentages are noted below 30 years and above 60. This can be expected from people above 60. 

Age can be an impactful predictor. 

$~$

### C. Job

The next predictor is job, or rather professional status since we also see
retired people, students and unemployed people. Let's have a look at a barplot.

$~$

```{r Graph about job}
graph <- bank_train %>% dselect(job) %>% group_by(job) %>% 
  summarize(n = n()) %>% as.data.frame() %>% mutate(job = reorder(job, desc(n))) %>%
  ggplot(aes(job, n)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Customers per Professional Status in Training Set") +
  xlab("Professional Status") + ylab("Count of Customers") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12), 
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

$~$

Some subgroups, such as blue-collars and  managers, are much more populated than others. Let's build up a table, adding percentages of subscribers per professional status in descending order. 

$~$

```{r Table about job}
tab <- bank_train %>% dselect(y, job) %>% mutate(y = as.numeric(y) - 1) %>% 
  group_by(job) %>% summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  mutate(percent_yes = format(percent_yes, digits = 0, nsmall = 0)) %>%
  arrange(desc(percent_yes)) %>%
  rename(Professional_Status = job, Number_of_Customers = n, 
         Subscription_Percentage = percent_yes) %>% as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
  full_width = F, font_size = 16) %>% column_spec(1:3, width = "2in")
rm(tab)
```

$~$

Retired people and students are above 25% and the subgroup with "unknown" professional status is first in subscription percentage with 30%. Blue-collars have the lowest subscription percentage, with 6%.

Partially, this intersects and corroborates insight from age: extremes in age
have higher subscription percentages; this is similar information about retired people 
and students. 

But age and professional status are no mere duplicates: the subgroup of retired people and the subgroup of customers over 60 are far from completely intersecting: there are 76 customers above 60 and 131 retired people! Moreover, there are many other subgroups! 

This predictor could also be impactul in predicting.

$~$

### D. Marital Status

The table hereunder gives the concentration of customers per marital status, distinguishing married people, single and divorced people, widows and widowers being considered as divorced people. It also gives the average subscription percentages per subgroup. 

$~$

```{r Table about marital status}
tab <- bank_train %>% dselect(y, marital) %>% 
  mutate(y = as.numeric(y) - 1) %>% group_by(marital) %>%
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  mutate(percent_yes = format(percent_yes, digits = 2, nsmall = 0)) %>%
  arrange(desc(percent_yes)) %>%
  rename(Marital_Status = marital, Number_of_Customers = n, 
         Subscription_Percentage = percent_yes) %>% as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 14) %>% 
  column_spec(1:3, width = "2in")
rm(tab)
```

$~$

The difference in subscription percentage is about 2-3 percentage points between groups, the lowest percentage being 10% for married people and the highest percentage being 15% for single people, divorced people being in the middle. This can indicate potential impactfulness from marital status. 

$~$

### E. Education

The last predictor from customers' non-financial status is the education level,  distinguishing primary, secondary, tertiary levels and unknown. 

$~$

```{r Table about education}
tab <- bank_train %>% dselect(y, education) %>% 
  mutate(y = as.numeric(y) - 1) %>% group_by(education) %>%
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  mutate(percent_yes = format(percent_yes, digits = 1, nsmall = 0)) %>%
  arrange(desc(percent_yes)) %>%
  rename(Education_Level = education, Number_of_Customers = n, 
         Subscription_Percentage = percent_yes) %>% as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 14) %>% 
  column_spec(1:3, width = "2in")
rm(tab)
```

$~$

Customers with tertiary education have on average a subscription percentage of 15%. The other subgroups are around 9-10%. That dichotomy seems promising in terms of predictive power from that predictor. 

Is there a link with professional status, e.g with managers? 756 customers with tertiary education have an average percentage of 15; 539 managers have an average percentage of 14 (see hereinabove). Is there a strong intersection between the two groups? Actually, 431 customers are managers with tertiary education with an average subscription percentage 
of 15, i.e. close to the average for managers and the average for people with tertiary education. So, most managers have a tertiary education level. 

This does not mean that education and professional status are duplicate information. Indeed, the intersection is far from complete even for tertiary education and managers and, moreover, there are many other subgroups according to education and to professional status. 

$~$

### F. Credit Default

The first predictor from the financial status is credit default. 

$~$

```{r Table about default}
tab <- bank_train %>% dselect(y, default) %>% 
  mutate(y = as.numeric(y) - 1) %>% group_by(default) %>%
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  mutate(percent_yes = format(percent_yes, digits = 1, nsmall = 0)) %>%
  arrange(desc(percent_yes)) %>%
  rename(Credit_Default = default, Number_of_Customers = n, 
         Subscription_Percentage = percent_yes) %>% as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 14) %>% 
  column_spec(1:3, width = "2in")
rm(tab)
```

$~$

There is a difference in subscription percentage between customers in credit default and customers who are not. But the subgroup of customers in default is limited. Will this predictor be impactful? 

$~$

### G. Account Balance

The range between the minimum of -3,313 and the maximum of 71,188 is rather broad. Let's have a look at a table with subgroups and average subscription percentages.  

$~$

```{r Table about balance}
tab <- bank_train %>% dselect(y, balance) %>% 
  mutate(balance = cut(as.numeric(balance), 
         breaks = c(- 5000, - 1000, 0, 0.1, 1000, 5000, 10000, 15000, 75000), 
         right = FALSE, dig.lab = 5)) %>%
  mutate(y = as.numeric(y) - 1) %>% 
  group_by(balance) %>% summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  mutate(percent_yes = format(percent_yes, digits = 1, nsmall = 0)) %>%
  rename(Account_Balance = balance, Number_of_Customers = n, 
         Subscription_Percentage = percent_yes) %>% as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
  full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(tab)
```

$~$

In the table above, tranches of account balances have not been sorted by descending order 
of subscription percentage because subscription percentages show a remarkable 
pattern, being rather centered: between 1,000 and 10,000, people are more prone to subscribe a term deposit than people with smaller or bigger account balances. 

Average subscription percentages vary between 7 and 16, without taking into consideration a zero average percentage for a very small subgroup of people with negative imbalances below -1,000. 

Account balance can be an effective predictor.  

$~$

### H. Home Loan

The third financial predictor is about the customer having a home loan or not having any.

$~$

```{r Table about home loan}
tab <- bank_train %>% dselect(y, housing) %>% 
  mutate(y = as.numeric(y) - 1) %>% group_by(housing) %>% 
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  mutate(percent_yes = format(percent_yes, digits = 1, nsmall = 0)) %>%
  rename(Home_Loan = housing, Number_of_Customers = n, 
         Subscription_Percentage = percent_yes) %>% as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>% 
  column_spec(1:3, width = "2in")
rm(tab)
```

$~$

Clear percental difference between the two subgroups, i.e. customers with a home loan and customers without any. Moreover, both categories are substantively populated. This predictor seems a promising one.   

$~$

### I. Personal Loan

The fourth financial predictor is about the customer having a personal loan or not having any.

$~$

```{r Table about personal loan}
tab <- bank_train %>% dselect(y, loan) %>% 
  mutate(y = as.numeric(y) - 1) %>% group_by(loan) %>% 
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  mutate(percent_yes = format(percent_yes, digits = 1, nsmall = 0)) %>%
  rename(Personal_Loan = loan, Number_of_Customers = n, 
         Subscription_Percentage = percent_yes) %>% as.data.frame() %>%
  as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 14) %>% 
  column_spec(1:3, width = "2in")
rm(tab)
```

$~$

There is a clear percental difference. This predictor seems promising even if the subgroup with a personal loan is quantitatively more limited. 

$~$

### J. Medium of Communication in this Campaign

Here is the first predictor linked directly to this marketing campaign, i.e. the 
marketing campaign whose results are to be predicted in this project.

$~$

```{r Table about medium of communication}
tab <- bank_train %>% dselect(y, contact) %>% mutate(y = as.numeric(y) - 1) %>% 
  group_by(contact) %>% summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  mutate(percent_yes = format(percent_yes, digits = 1, nsmall = 0)) %>%
  rename(Medium_of_Communication = contact, Number_of_Customers = n, 
         Subscription_Percentage = percent_yes) %>% as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>% 
  column_spec(1:3, width = "2.5in")
rm(tab)
```

$~$

There are three subgroups: customers contacted on a cellular phone, customers contacted on a landline phone and people for whom that piece of information is unknown. In terms of average subscription percentage, there is a clear dichotomy between customers for whom that piece of information is available and customers for whom it is not. The respective levels are 14% and 5%. 

This predictor seems very promising. 

$~$

### K. Day of the Month of Last Contact with Customer in this Campaign

The next predictor is the day of the month of the last contact with the customer. Maybe there are temporal patterns.   

First, let's see the distribution of the days of the month of the last contact.

$~$

```{r Graph about day of month of last contact}
graph <- bank_train %>% dselect(day) %>% group_by(day) %>% 
  summarize(n = n()) %>% as.data.frame() %>% 
  ggplot(aes(day, n)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Customers per Day of the Month of Last Contact") +
  xlab("Day of the Month") + ylab("Count of Customers") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12))
graph
rm(graph)
```

$~$

The number of customers per day of the month of the last contact is rather volatile. There are more contacts in the middle of months. Let's have a look at the variation in subscription percentage between days of the month.

$~$

```{r Second graph about day of month of last contact: subscription percentage}
graph <- bank_train %>% dselect(y, day) %>% 
  mutate(y = as.numeric(y) - 1) %>% group_by(day) %>%
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>% as.data.frame() %>% 
  ggplot(aes(day, percent_yes)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Subscription Percentage per Day of Last Contact") +
  xlab("Day of the Month") + ylab("Subscription Percentage") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12))
graph
rm(graph) 
```

$~$

It also seems rather volatile. It looks partially counter-cyclical with respect to the number of customers per day. Will that predictor be influential? 

$~$

### L. Month of the Last Contact with Customer in this Campaign

The next predictor is also directly linked to this marketing campaign: it is
the month of the last contact in this campaign. Is there a temporal pattern for the month of the last contact? 

$~$

```{r Graph about the month of the last contact: number of customers}
bank_train$month_ord = factor(bank_train$month, ordered = TRUE, levels = c("jan", "feb", 
  "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec"))
graph <- bank_train %>% dselect(month_ord) %>% group_by(month_ord) %>% 
  summarize(n = n()) %>% as.data.frame() %>% 
  ggplot(aes(month_ord, n)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Customers per Month of Last Contact") +
  xlab("Month") + ylab("Count of Customers") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12), 
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

$~$

The last contact did often take place from May until August. But how is this related to the percentage of subscription?

$~$

```{r Graph about the month of the last contact: average subscription percentages}
graph <- bank_train %>% dselect(y, month_ord) %>% 
  mutate(y = as.numeric(y) - 1) %>% group_by(month_ord) %>% 
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  as.data.frame() %>%
  ggplot(aes(x = month_ord, y = percent_yes)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Subscription Percentage per Month of Last Contact") +
  xlab("Month") + ylab("Subscription Percentage") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12), 
        axis.text.y = element_text(size = 12))
graph
rm(graph)
bank_train$month_ord <- NULL
```

$~$

The subscription percentage per month of last contact looks generally counter-cyclical with respect to the number of customers: there are several higher subscription percentages outside the period May-August.  

$~$

### M. Duration in Seconds of Last Contact with Customer

It is the duration of the contact whose results are to be predicted. 

There is a strong relationship between contact duration and subscription probability. But using it here would be a bias. Indeed, the duration of the contact whose results are to be predicted is not known before that contact. Consequently, that piece of information cannot be considered as a predictor of subscription since it is known only after contact. 

For that reason, and in complete agreement with the recommendations available on the 
site, this variable will not be taken into account. It will be discarded from the training set, from the test set and from the validation set. 

$~$

### O. Number of Contacts with Customer during this Campaign

The number of contacts per customer can vary very much as the table hereunder shows it. For most customers, the number of contacts in this campaign is one, two or three. But the number can attain 50! That range is rather broad. 

Is the subscription percentage in a subgroup related to the number of contacts? 

$~$

```{r Table about contact number and subscription percentage in this campaign}
tab <- bank_train %>% dselect(y, campaign) %>% 
  mutate(campaign = cut(as.numeric(campaign), breaks = c(1, 2, 3, 5, 10, 51), 
                        right = FALSE, dig.lab = 5)) %>%
  mutate(y = as.numeric(y) - 1) %>% group_by(campaign) %>% 
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  mutate(percent_yes = format(percent_yes, digits = 1, nsmall = 0)) %>%
  rename(Number_of_Contacts_in_this_Campaign = campaign, Number_of_Customers = n, 
         Subscription_Percentage = percent_yes) %>% as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>% 
  column_spec(2:3, width = "2in")
rm(tab)
```

$~$

In the table above, there are five subgroups constituted on basis of the number of contacts in this campaign. In the subgroup of customers with one contact and in the subgroup with two contacts, the subscription percentage is 13%. The subscription percentage decreases somewhat in the subgroup with three or four contacts. But in the subgroups with more than four contacts, the subscription percentage drops dramatically, landing at 5% above 9 contacts. 

Consequently, that table shows, at the level of subgroups, a clear negative link between number of contacts and subscription percentage, even if we perfectly know that correlation is not causation! 

This might make sense, though! If there have already been e.g. seven or eight contacts, I can imagine that there is probably some hesitation from the customer!

$~$

### P. Number of Days since Last Contact with Customer in Previous Campaign

This is the first predictor related to previous marketing action, thus before "this" campaign. 

$~$

```{r Number of Days since Last Contact with Customer in Previous Campaign}
tab <- bank_train %>% dselect(y, pdays) %>% 
  mutate(pdays = cut(as.numeric(pdays), 
                     breaks = c(- 1, 0, 0.1, 50, 100, 250, 500, 1000), 
                     right = FALSE, dig.lab = 5)) %>%
  mutate(y = as.numeric(y) - 1) %>% group_by(pdays) %>% 
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  mutate(percent_yes = format(percent_yes, digits = 1, nsmall = 0)) %>%
  rename(Days_since_Last_Contact_in_Previous_Campaign = pdays, 
         Number_of_Customers = n, Subscription_Percentage = percent_yes) %>% 
  as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>% 
  column_spec(2:3, width = "2.5in")
rm(tab)
```

$~$

In the table above, we can see that the number of days since the last contact before this campaign is very often - 1! Actually, the value - 1  means that there had been no contact before this campaign. It is so for the vast majority of customers, i.e. for 2,064 customers out of 2,531! 

This is correlated with the average subscription percentages (we can state it even if we know that correlation is not causation!). There is a major dichotomy between the subgroup "no contact before this campaign" and the subgroup "one or more contacts before this campaign". Indeed, customers without any contact in a previous marketing campaign respond favorably on average in 9% of cases against 15%, 20%, 33% or even 47% in case of previous contact(s). Is this so surprising? Selling on the phone to customers who have not been contacted in that way before can prove to be a tough challenge. A promising piece of information can be the dichotomy between "previous contact" and "no previous contact". 

In case of previous contact, less than 100 days seems more promising than more than 100 days, which also makes sense. 

$~$

### Q. Number of Contacts with Customer before this Campaign

This predictor will be partially redundant with respect to the previous one. Indeed, here, once again, we will find the scenario of customers not having been contacted before the campaign under review. 

$~$

```{r Number of Contacts with Customers before this Campaign}
tab <- bank_train %>% dselect(y, previous) %>% 
  mutate(previous = cut(as.numeric(previous), breaks = c(0, 0.1, 5, 10, 26), 
                        right = FALSE, dig.lab = 5)) %>%
  mutate(y = as.numeric(y) - 1) %>% group_by(previous) %>% 
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>% 
  mutate(percent_yes = format(percent_yes, digits = 1, nsmall = 0)) %>%
  rename(Number_of_Previous_Contacts = previous, Number_of_Customers = n, 
         Subscription_Percentage = percent_yes) %>% as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>% 
  column_spec(1:3, width = "2.5in")
rm(tab)
```

$~$

In the table above, we can find back the 2,064 customers who had not been contacted before the campaign under analysis. 

Zero previous contact is a very negative factor. This duplicates information from previous predictor, i.e. the number of days since the last contact before this campaign. In the case of the previous predictor (number of days), the average subscription percentages in subgroups with previous contact was rather "bumpy" from one subgroup to another. In the case of the number of previous contacts, there appears less variation among subgroups with previous contact as they are constituted.

$~$

### R. Outcome from Previous Marketing Campaign

This is the last predictor. Of course, we will find once again the same piece of information about the 2,064 customers who had not been contacted before. But maybe there is some additional information. 

$~$

```{r Table about outcome from preious campaign}
tab <- bank_train %>% dselect(y, poutcome) %>% 
  mutate(y = as.numeric(y) - 1) %>% group_by(poutcome) %>% 
  summarize(n = n(), percent_yes = (1 - (sum(y) / n)) * 100) %>%  
  mutate(percent_yes = format(percent_yes, digits = 1, nsmall = 0)) %>%
  rename(Outcome_from_Previous_Campaign = poutcome, Number_of_Customers = n, 
         Subscription_Percentage = percent_yes) %>% as.data.frame()
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>% 
  column_spec(1:3, width = "1.5in")
rm(tab)
```

$~$

There is, indeed, some very interesting new information. Apart from the 2,064-customer subgroup, the subgroup of customers who were previously contacted and who previously subscribed term deposits shows an extraordinary subscription rate of 60%. That looks like a piece of information with very impactful predictive power.

$~$

## VI. Insights from Exploratory Analysis and Data Preparation

### A. Low Prevalence of "deposit" in the Target Variable y

There are 88.5% "no_deposit" responses in the training set. Predicting "no_deposit" for all customers would already deliver an accuracy level of 88.5%. But this would also deliver a sensitivity of zero! 

This is at the opposite of the objective, which consists in reaching one of the equivalent alternatives from the objective table reprinted hereunder (equivalent from the point of view of reaching the objective).

$~$

```{r Reprinting objective table.}
kable(objective_table, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16)
```

$~$

Alternative 1 requires a sensitivity of 0.76, alternative 2 a sensitivity of 0.75 and alternative 3 a sensitivity of 0.74. Consequently, a sensitivity level of zero would be absolutely incompatible with the objective in this project. 

Even an accuracy level of e.g. 0.92 would be compatible with any sensitivity value in a range from 0.304 to 1, consequently with a lot of sensitivity values smaller than 0.74. The value 0.304 is coming from (92 - 88.5) / 11.5. 

In consequence, accuracy is not the right performance measure in this challenge. It is about percentage of subscribers reached and global coverage reduction (please see hereinabove the section "II. Objective of the Project and Terminology").  

$~$

### B. Insights about Predictors

Each predictor provides a segmentation of customers into subgroups, each subgroup having its average subscription percentage. In the exploratory analysis, some remarkable subgroups have been noted from the point of view of subscription percentages:

-	customers younger than 30 or older than 60 have on average higher subscription percentages than the other subgroups formed on the basis of age;

-	the same holds for students, retired people and customers with "unknown" professional status while, on the contrary, the subgroup of blue-collars has the lowest subscription percentage with respect to other "professional" subgroups;

-	the subgroup of married people has a lower subscription percentage than single and divorced people;

-	on the contrary, customers with tertiary education form a subgroup with the highest subscription percentage with respect to other education levels; 

-	it is the same for customers with an account balance between 1,000 and 10,000;

-	the subgroups of customers with home loans or personal loans have lower subscription percentages;

-	the same holds for customers for whom the medium of communication is unknown!

-	we can see higher subscription percentages than average when the last contact in this campaign is in March, September, October or December and lower from May until August;

-	the subgroup of customers who have been contacted but less than five times in the campaign under review has a higher average subscription percentage; 

-	segmentation  on the basis of previous marketing action indicates the following: customers who favorably responded in a previous marketing campaign have on average a much higher subscription rate ; the same holds, but to a lesser extent, for customers that had been contacted in a previous marketing campaign less than 100 days before the last contact in this campaign; but customers who had not been contacted in previous marketing action, which is a vast majority, have a much lower subscription percentage on average than the other subgroups. 

Duration should not be present as a predictor. "this attribute highly affects 
the output target  (e.g., if duration = 0 then y = "no_deposit"). Yet, the duration 
is not known before a call is performed. Also, after the end of the call y is 
obviously known. Thus, this input should only be included for benchmark purposes 
and should be discarded if the intention is to have a realistic predictive model." 
(https://archive.ics.uci.edu/ml/datasets/Bank+Marketing , retrieved on 2019-04-16)

Therefore, duration will be discarded from the training set, the test set and the validation set.

```{r Removing duration}
bank_train$duration <- NULL
bank_test$duration <- NULL
bank_val$duration <- NULL
```

Let's express a caveat about some subgroups and their subscription percentages: some subgroups are rather small and this might affect their predictive power on the test set or the validation set.  

We are now ready to turn to modeling on the training set.

$~$

## VII. Analysis, Modeling and Insights on the Training Set

Let's use the training set to try several machine learning models available in the caret package. They will be tested later on the test set before a final model is chosen and then validated on the validation set. 

$~$

### A. Running 4 Machine Learning Models on the Training Set

#### 1. Picking up Models and Getting Results

Numerous machine learning models have been tried, such as qda, knn, kknn, svmLinear, svmRadial, svmRadialCost, svmRadialSigma, rpart, ranger, wsrf, Rborist, monmlp, etc. After many trials, four models have been selected: glm, lda, rf and gbm. 

The four selected models will be run with the train() function from the caret package.

For each model, two measures will be computed: 

- the percentage of subscribers reached: sensitivity or recall multiplied by 100;

- the global coverage reduction: percentage of customers for whom the model prediction is "no_deposit".

These are the two criteria to use to evaluate model performance. 

$~$

```{r Running models on the training set and getting performance in table.}

# Listing models and loading packages.
models <- c("glm", "lda", "rf", "gbm")
if(!require(MASS)) install.packages("MASS", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")

# Training  selected models with the train() function.
part <- c("glm", "lda", "rf")
fits <- lapply(part, function(model){
  set.seed(1)
  train(y ~ ., method = model, data = bank_train)
  }) 
set.seed(1)
gbm <- train(y ~ ., method = "gbm", data = bank_train, verbose = FALSE)
fits[[4]] <- gbm
names(fits) <- models
rm(part, gbm)

# Getting fitted values from all models.
fitted <- sapply(fits, function(object) predict(object)) %>% as.data.frame()

# Calculating number of subscribers and of customers in the training set.
subs_number <- bank_train %>% filter(y == "deposit") %>% summarize(n = n()) %>% .$n
cust_number <- nrow(bank_train)

# Calculating percentages of subscribers reached. 
len_mod <- length(models)
seq_mod <- 1:len_mod
sensitivities <- sapply(seq_mod, function(i) 
  sensitivity(as.factor(fitted[, i]), bank_train$y))
subs_reached_pct <- as.vector(sensitivities) * 100

# Calculating percentages of not contacted customers.
posPredValues <- sapply(seq_mod, function(i) 
  posPredValue(as.factor(fitted[, i]), bank_train$y))
TP <- round(subs_number * subs_reached_pct / 100)
FP <-  TP * (1 - posPredValues) / posPredValues
cust_not_contacted_pct <- (cust_number - TP - FP) / cust_number * 100
rm(sensitivities, posPredValues)

# Table of results called "results_tab_train_50" because it is calculated with the default probability threshold 0.50 to decide whether it is "deposit" or "no_deposit". 
results_tab_train_50 <- data.frame(Model = as.character(models), 
  Subscribers_Reached = subs_reached_pct, 
  Global_Coverage_Reduction = cust_not_contacted_pct) %>%
  arrange(Subscribers_Reached) %>% as.data.frame()
tab <- results_tab_train_50 %>%  
  mutate(Subscribers_Reached = 
         format(Subscribers_Reached, digits = 0, nsmall = 0)) %>% 
  mutate(Subscribers_Reached = paste(Subscribers_Reached, "%", sep = " ")) %>%
  mutate(Global_Coverage_Reduction = 
         format(Global_Coverage_Reduction, digits = 0, nsmall = 0)) %>%
  mutate(Global_Coverage_Reduction = paste(Global_Coverage_Reduction, "%", sep = " "))
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>% 
  column_spec(1:2, width = "1.5in") 
rm(tab)
```

$~$

#### 2. Analysis of Results on the Training Set, Insights and Ways Forward

There is a very broad range of "predictive" quality on the training set. 

On the one hand, in percentage of subscribers reached, only rf meets the target, with 100% against 14%, 15% and 24% for the other ones. On the other hand, in global coverage reduction, all models largely meet the target. Globally, when taking both criteria into account, as required, **rf meets the target, the other 3 models are far from it**.

Actually, there are **two caveats**: three models do not reach the target; the model that does might be overfitting.

Let's start with the second caveat. The performance from rf in percentage of subscribers reached is combined with high global coverage reduction! Let's check up on the probability of model rf overfitting by building up a table with FP and FN. 

$~$

```{r Check-up for overfitting from rf}
FN <- subs_number - TP 
check_up_tab <- data.frame(Model = as.character(models), FP = FP, FN = FN) %>% 
  as.data.frame() %>%  mutate(FP = format(FP, digits = 0, nsmall = 0)) %>% 
  mutate(FN = format(FN, digits = 0, nsmall = 0)) 
kable(check_up_tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>% 
  column_spec(1:3, width = "1.5in")
rm(FN, FP, TP, check_up_tab)
```

$~$

For rf model, there is no false positive and no false negative! Consequently, accuracy, sensitivity, positive predictive value, etc. are all equal to 1! Isn't it too perfect? Running rf on the test set will bring additional information about possible overfitting. 

Let's turn now to the first caveat, i.e. 3 models missing the target area. Performance from the various models can be visualized on the following graph.  

$~$

```{r Graph with first results from the 4 models on the training set}
graph <- results_tab_train_50 %>%  
  ggplot(aes(x = Subscribers_Reached, y = Global_Coverage_Reduction, label = Model)) + 
  geom_point(size = 3, color = "#007ba7") + geom_text(nudge_y = 5) +
  geom_rect(aes(xmin = 75, xmax = 100, ymin = 50, ymax = 100), 
            fill = "#9bc4e2", alpha = 0.1) +
  ggtitle("First Results on Training Set") +
  xlim(0, 105) + xlab("Subscribers Reached (%)") + 
  ylim(0, 105) + ylab("Global Coverage Reduction (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12))
graph
rm(graph)
rm(cust_not_contacted_pct, subs_reached_pct, FP, results_tab_train_50)
```

$~$

The target area is approximately represented by the light blue rectangle in the upper right corner. 

Each model is represented by a point with its label. The coordinates of the points are calculated with the default threshold 0.5 to decide whether the fitted value for a customer should be "deposit" or "no_deposit". 

Performance should be upgraded for the 3 models that do not at all meet the target. 

$~$

### B. Dimension Reduction

I have first tried dimension reduction. I have limited the number of predictors by using information from the glm and the rf models. 

From the glm model, I have taken the 14 predictors that had some statistical significance (p-value < 0.1). From the rf model, I have taken the first 20 predictors delivered by the function varImp(). 

I have added a predictor. 

This has given the following list: age, jobblue-collar, jobretired, jobstudent, jobtechnician, jobunknown, maritalmarried, educationsecondary, educationtertiary, balance, housingyes, loanyes, contactunknown, day, monthjan, monthmay, monthjul, monthaug, monthoct, monthnov, campaign, pdays, previous, poutcomesuccess, poutcomeother.

This list largely intersects the insights from the exploratory analysis. 

In fact, with this dimension reduction, model performance has worsened. Let's explore another avenue of research to improve performance.   

$~$

### C. Tuning the Probability Threshold

#### 1. Retrieving and Analyzing Probabilities

Let's retrieve probabilities for each model and analyze e.g. probabilities from the lda model.  

$~$

```{r Retrieving probabilities from all models and visualizing probabilities from the lda model.}

# Retrieving probabilities
prob_train <- lapply(fits, function(object) predict(object, type = "prob"))
seq <- seq(1, (len_mod * 2) - 1, 2)
prob_train <- prob_train %>% as.data.frame() %>% dselect(seq)
names(prob_train) <- gsub(".deposit", "", names(prob_train))
rm(seq)

# Probabilities from lda model
graph_prob_lda <- prob_train %>% dselect(lda) %>% ggplot(aes(lda)) + 
  geom_histogram(bins = 30, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("lda Model") +
  xlab('Probabilities of "deposit"') + ylab("Count of Customers") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
    axis.text.x = element_text(size = 12), axis.text.y =element_text(size = 12))
graph_prob_lda
rm(graph_prob_lda)
```

$~$

On the graph above, there are very few probabilities larger than 0.5. To select more customers, and hopefully more subscribers, the probability threshold has to be lowered. 

$~$

#### 2. Optimizing the Probability Threshold on the Training Set

Cross-validation will be performed by tuning the probability threshold below 0.5. This will be done for all models, even if the rf model already meets the target on the training set. Let's produce for each model and for each threshold the two performance measures that are applied in this challenge. The table hereunder shows performance measures for a limited number of thresholds values. 

$~$

```{r Tuning probability threshold and getting results for each threshold and model.}
cutoffs <- seq(0.05, 0.5, 0.001)
len_cut <- length(cutoffs)
seq_cut <- 1:len_cut

# Creating data frame to later receive sensitivity values for all thresholds/models
new_sensitivities <- data.frame(matrix(nrow = len_cut, ncol = len_mod) * 1)
names(new_sensitivities) <- models

# Creating data frame to later receive precision values for all thresholds/models
new_posPredValues <- data.frame(matrix(nrow = len_cut, ncol = len_mod) * 1)
names(new_posPredValues) <- models

# Calculating sensitivity and precision values. 
for (i in seq_cut) {
  for (j in seq_mod) { 
    refitted <- ifelse(prob_train[, j] > cutoffs[i], "deposit", "no_deposit") %>%
      factor(levels = levels(bank_train$y))
    new_sensitivities[i, j] <- confusionMatrix(refitted, bank_train$y)$byClass[1]
    new_posPredValues[i, j] <- confusionMatrix(refitted, bank_train$y)$byClass[3]
    } 
}
rm(i, refitted)

# Percentages of subscribers reached for all thresholds/models
new_subs_reached_pct <- new_sensitivities * 100

# Percentages of customers not contacted for all thresholds/models. 
new_FP <- data.frame(matrix(nrow = len_cut, ncol = len_mod) * 1)
for (i in seq_cut) {
  for (j in seq_mod) { 
    new_FP[i, j] <- round(subs_number * new_subs_reached_pct[i, j] / 100) *  
      (1 - new_posPredValues[i, j]) / new_posPredValues[i, j]
  } 
} 
new_cust_not_contacted <- cust_number - 
  round(subs_number * new_subs_reached_pct / 100) - new_FP
new_cust_not_contacted_pct <- new_cust_not_contacted / cust_number * 100
rm(new_FP)

# Table of results for all probabilitiy thresholds and all models. 
evaluation_table_train_all <- 
  data.frame(matrix(nrow = len_cut, ncol = len_mod * 2) * 1)
seq_odd <- seq(1, (len_mod * 2) - 1, 2)
colnames(evaluation_table_train_all)[seq_odd] <- c(paste(models, "s", sep = "_"))
seq_even <- seq(2, (len_mod * 2) , 2)
colnames(evaluation_table_train_all)[seq_even] <- c(paste(models, "r", sep = "_"))
rm(seq_odd, seq_even)
for (i in 1:len_mod) {
  evaluation_table_train_all[(i * 2) - 1] <- new_subs_reached_pct[, i]
  evaluation_table_train_all[(i * 2)] <- new_cust_not_contacted_pct[, i]
} 
evaluation_table_train_all <- evaluation_table_train_all %>% 
  mutate(Thresh = as.character(round(cutoffs, 5))) %>% 
  dselect(Thresh, everything())

# Let's print part of the table of results: one out of 10 threshold values.
seq <- seq(1, 451, 10)
tab <- evaluation_table_train_all[seq, ] %>% as.data.frame()
kable(tab, "html", align = "c", digits = 0, nsmall = 0) %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
  full_width = F, font_size = 16) %>% column_spec(1:9, width = "1.5in")
rm(seq, tab)
```

$~$

In the table above, "glm_s" heads the column of percentages of subscribers reached ("s" standing of course for "subscribers") for the glm model and for some thresholds. The equivalent holds of course for lda, etc. 

"glm_r" heads the column of global coverage reductions ("r" standing of course for "reduction") for the glm model and for the same thresholds. The equivalent holds of course for lda, etc. 

For the concepts of percentage of subscribers reached and of global coverage reduction, please see above the section "II. Objective of the Project and Terminology".
 
Now, the 4 models meet the target: glm, lda, rf and gbm. Let's graphically visualize. 

$~$

```{r Graph with performance curves from the 4 individual models on the training set}
graph <- evaluation_table_train_all %>% ggplot() + 
  geom_point(aes(x = glm_s, y = glm_r), size = 1, color = "green") +
  geom_point(aes(x = lda_s, y = lda_r), size = 1, color = "yellow") + 
  geom_point(aes(x = rf_s, y = rf_r), size = 2, color = "red", alpha = 0.3) +
  geom_point(aes(x = gbm_s, y = gbm_r), size = 1, color = "blue") +
  geom_rect(aes(xmin = 75, xmax = 100, ymin = 50, ymax = 100), 
            fill = "#9bc4e2", alpha = 0.01) +
  ggtitle("Training Set: glm(green) - lda(yellow) - rf(red) - gbm(blue)") + 
  xlim(0, 105) + xlab("Subscribers Reached (%)") + 
  ylim(0, 105) + ylab("Global Coverage Reduction (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
    axis.text.x = element_text(size = 12), axis.text.y =element_text(size = 12))
graph
rm(graph, cust_number, subs_number, cutoffs, len_cut, seq_cut, i, j, fitted, 
   prob_train, evaluation_table_train_all, new_cust_not_contacted, 
   new_cust_not_contacted_pct, new_posPredValues, new_sensitivities, 
   new_subs_reached_pct)
```

By the way, in the graph above the diameter and transparency of points have been chosen for visual clarity and not for reasons linked to data. 

$~$

#### 3. Analysis of Results on the Training Set, Insights and Ways Forward

The graph above shows that the 4 individual models meet the target, in different ways.

The rf model, represented as a vertical red line on the right side of the graph, fully meets the objective: all points from the rf model are in the target area. The higher the threshold, the higher the points. There can be overfitting. 

The gbm model is represented by bright blue points. The series of points moves up and left in a curvy way as the threshold increases. Points are rather close to each other at the top of the curve but are most distant from each other in lower parts of the series. 

The glm model, represented in green, also passes through the target area, a little bit more "off-center" than gbm.   

The lda model is represented in yellow. It slightly touches the target area.  

The 4 models will be run on the test set.  

By the way, I have not built up an ensemble model with the four successful models because of the patterns of the series representing the rf model. 

$~$

## VIII. Testing and Insights on the Test Set 

On the test set, let's use the predict() function from caret to retrieve probabilities of "deposit". Let's then select a sequence of thresholds for probabilities, compute for each threshold and for each model the percentage of subscribers reached and the global coverage reduction, analyze results and possibly move forward. 

$~$

### A. Running the 4 Machine Learning Models on the Test Set - Insights

$~$

```{r Probabilities for the test set - tuning threshold - table of results}
# Probabilities
prob_test <- lapply(fits, function(object) 
  predict(object, newdata = bank_test, type = "prob"))
seq <- seq(1, (length(models) * 2) - 1, 2)
prob_test <- prob_test %>% as.data.frame() %>% dselect(seq)
names(prob_test) <- gsub(".deposit", "", names(prob_test))
rm(seq)

# Sequence of thresholds 
cutoffs <- seq(0.05, 0.5, 0.001)
len_cut <- length(cutoffs)
seq_cut <- 1:len_cut

# Data frames of sensitivity and precision for all combinations models/thresholds
test_sensitivities <- data.frame(matrix(nrow = len_cut, ncol = len_mod) * 1)
names(test_sensitivities) <- models
test_posPredValues <- data.frame(matrix(nrow = len_cut, ncol = len_mod) * 1)
names(test_posPredValues) <- models
for (i in seq_cut) {
  for (j in seq_mod) { 
    refitted <- ifelse(prob_test[, j] > cutoffs[i], "deposit", "no_deposit") %>%
      factor(levels = levels(bank_test$y))
    test_sensitivities[i, j] <- confusionMatrix(refitted, bank_test$y)$byClass[1]
    test_posPredValues[i, j] <- confusionMatrix(refitted, bank_test$y)$byClass[3]
  } 
}
rm(i, j, refitted)

# Number of customers and of subscribers in test set
test_cust_number <- nrow(bank_test)
test_subs_number <- bank_test %>% filter(y == "deposit") %>% nrow()

# Data frame of percentages of subscribers reached for all combinations models/thresholds
test_subs_reached_pct <- test_sensitivities * 100

# Data frame of percentages of customers not contacted for all models/thresholds
test_FP <- data.frame(matrix(nrow = len_cut, ncol = len_mod) * 1)
names(test_FP) <- models
for (i in seq_cut) {
  for (j in seq_mod) { 
    test_FP[i, j] <- round(test_subs_number * test_subs_reached_pct[i, j] / 100) *  
      (1 - test_posPredValues[i, j]) / test_posPredValues[i, j]
  } 
} 
test_cust_not_contacted <- test_cust_number - 
  round(test_subs_number * test_subs_reached_pct / 100) - test_FP
test_cust_not_contacted_pct <- test_cust_not_contacted / test_cust_number * 100

# Table of results for all combinations models/thresholds 
evaluation_table_test <- data.frame(matrix(nrow = len_cut, ncol = len_mod * 2) * 1)
seq_odd <- seq(1, (len_mod * 2) - 1, 2)
colnames(evaluation_table_test)[seq_odd] <- c(paste(models, "s", sep = "_"))
seq_even <- seq(2, (len_mod * 2) , 2)
colnames(evaluation_table_test)[seq_even] <- c(paste(models, "r", sep = "_"))
rm(seq_odd, seq_even)
for (i in 1:len_mod) {
  evaluation_table_test[(i * 2) - 1] <- test_subs_reached_pct[, i]
  evaluation_table_test[(i * 2)] <- test_cust_not_contacted_pct[, i]
} 
evaluation_table_test <- evaluation_table_test %>% 
  mutate(Thresh = as.character(round(cutoffs, 5))) %>% 
  dselect(Thresh, everything())

# Let's print part of the table of results.
seq <- seq(11, 51, 1)
tab <- evaluation_table_test[seq,] %>% as.data.frame()
kable(tab, "html", align = "c", digits = 0, nsmall = 0) %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>%        column_spec(1:9, width = "1.5in")
rm(seq, tab)
```

$~$

2 models meet the target: rf and gbm. Let's graphically visualize. 

$~$

```{r Graph with performance on the test set}
graph <- evaluation_table_test %>% ggplot() + 
  geom_point(aes(x = glm_s, y = glm_r), size = 1, color = "green") +
  geom_point(aes(x = lda_s, y = lda_r), size = 1, color = "yellow") + 
  geom_point(aes(x = rf_s, y = rf_r), size = 2, color = "red", alpha = 0.2) +
  geom_point(aes(x = gbm_s, y = gbm_r), size = 1.5, color = "blue") +
  geom_rect(aes(xmin = 75, xmax = 100, ymin = 50, ymax = 100), 
            fill = "#9bc4e2", alpha = 0.01) +
  ggtitle("Test Set: glm(green) - lda(yellow) - rf(red) - gbm(blue)") + 
  xlim(0, 105) + xlab("Subscribers Reached (%)") + 
  ylim(0, 105) + ylab("Global Coverage Reduction (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
    axis.text.x = element_text(size = 12), axis.text.y =element_text(size = 12))
graph
rm(graph)
```

$~$

2 models meet the target: rf and gbm. 

gbm passes through the target area with 5 points. 

rf passes through the target area with 22 points. But it has also shown important variation from the training set to the test set with seeming overfitting on the training set. 

glm and lda no longer cross the target area. glm is closer to the target area than lda. 

Let's build an ensemble model to try to ensure more security in reaching the target area on the validation set. Indeed, rf and gbm only slightly scratch a corner of the target area. Moreover, rf shows large variation between training set and test set. 

$~$

### B. Ensemble Model and Insights

I have tried several combinations of individual models on the test set. I have opted for the combination of glm, rf and gbm. 

For each threshold, for each observation from the test set, a majority vote is organized: for each threshold, the predicted value for a customer is "deposit" if at least two out of the three models give for that customer a probability of "deposit" larger than the threshold. Consequently, the code will generate 451 series of 633 predicted values since there are 451 thresholds values between 0.05 and 0.5 with increments of 0.001 and 633 rows in bank_test. 

For each series of predicted values, two performance measurements will be calculated: the percentage of subscribers reached and the global coverage reduction. 

Here is a table of results comparing performance from the best performing individual models (glm, rf and gbm) and from the ensemble model. 

$~$

```{r Ensemble model on the test set and table of results with glm, rf and gbm}
# Data frame with sensitivity and precision for all thresholds on ensemble model
test_ensemble <- data.frame(threshold = cutoffs, ens_s = 1:len_cut, 
                            ens_r = 1:len_cut, stringsAsFactors = FALSE)
for (i in 1:len_cut) {
  dummy <- prob_test %>% dselect(c("glm", "rf", "gbm"))
  dummy[dummy > cutoffs[i]] <- 1
  dummy[dummy <= cutoffs[i]] <- 0
  dummy <- dummy %>% as.data.frame()
  votes <- rowSums(dummy)
  votes <- ifelse(votes >= 2, "deposit", "no_deposit") %>% 
    factor(levels = levels(bank_test$y))

  sensitivity <- confusionMatrix(votes, bank_test$y)$byClass[1]
  TP <- round(test_subs_number * sensitivity) 
  test_ensemble[i, 2] <-  sensitivity * 100
  
  precision <- confusionMatrix(votes, bank_test$y)$byClass[3]
  FP <- round(TP * (1 - precision) / precision)
  test_ensemble[i, 3] <-  (test_cust_number - TP - FP) / test_cust_number * 100
} 

# Table of results with the best 3 individual methods and the ensemble model 
evaluation_table_test_ens <- evaluation_table_test %>% 
  dselect("Thresh", "glm_s", "glm_r", "rf_s", "rf_r", "gbm_s", "gbm_r") %>%
  mutate(ens_s = test_ensemble$ens_s, ens_r = test_ensemble$ens_r) 

# Lets print part of the table of results.
seq <- seq(1, 51, 1)
tab <- evaluation_table_test_ens[seq, ] %>% as.data.frame()
kable(tab, "html", align = "c", digits = 0, nsmall = 0) %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
  full_width = F, font_size = 16) %>% column_spec(1:9, width = "1.5in")
rm(seq, tab)
```

$~$

The **glm model** does not meet the target; it shows stability between results on training and test sets. 

The **gbm model** meets the target for 5 probability thresholds between 0.083 and 0.087, with performance ranging between 78-49 and 74-54; it shows homogeneity between results on the training set and results on the test set.  

The **rf model** meets the target for 22 probability thresholds between 0.074 and 0.095 with performance ranging between 78-49 and 75-57; it shows huge variation between results on the training set and results on the test set; on the training set, sensitivity remained at 1 for all probability thresholds between 0.05 and 0.5. There was apparent overfitting. 

The **ensemble model** meets the target for 9 probability thresholds between 0.082 and 0.090, with performance ranging between 79-49 and 75-58. It reaches slightly higher scores than rf but is successful on less probability thresholds. Since the ensemble model is supported by three models, of which two (glm and gbm) are rather stable in results on training set and test set, I have opted for the ensemble model as final model to be applied to the validation set. 

For the ensemble model, the optimum threshold has been fixed at 0.086. How? It is the mean and the median of the thresholds that deliver performance that meet the target for the ensemble model. These probability thresholds range from 0.082 to 0.090, so the mean and the median are 0.086.

The glm model does not attain the target area but it improves the ensemble model performance.

Let's graphically visualize results from the ensemble model and from the 
three subcomponents, i.e. the glm, the rf and the gbm models. 

$~$

```{r Graph with glm, rf and gbm and the ensemble model on the test set}
graph <- evaluation_table_test_ens %>% ggplot() + 
  geom_point(aes(x = glm_s, y = glm_r), size = 1, color = "green") +
  geom_point(aes(x = rf_s, y = rf_r), size = 2, color = "red", alpha = 0.2) +
  geom_point(aes(x = gbm_s, y = gbm_r), size = 1.5, color = "blue") +
  geom_point(aes(x = ens_s, y = ens_r), size = 1.5, color = "yellow", alpha = 0.75) +
  geom_rect(aes(xmin = 75, xmax = 100, ymin = 50, ymax = 100), 
            fill = "#9bc4e2", alpha = 0.01) +
  ggtitle("Test Set: glm(green) - rf(red) - gbm(blue) - ensemble(yellow)") + 
  xlim(0, 105) + xlab("Subscribers Reached (%)") + 
  ylim(0, 105) + ylab("Global Coverage Reduction (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(size = 12), axis.text.y =element_text(size = 12))
graph
rm(graph, prob_test, test_cust_number, test_subs_number, test_cust_not_contacted, 
   test_cust_not_contacted_pct, test_ensemble, test_FP, test_posPredValues, 
   test_sensitivities, test_subs_reached_pct, cutoffs, TP, FP, i, j, votes,
   precision, sensitivity, len_cut, seq_cut, evaluation_table_test,
   evaluation_table_test_ens, dummy)
```

I had also developed another ensemble model. I had organized  majority vote among three individual series of predictions already individually optimized with three different thresholds, one series for glm, one for rf and one for gbm. This alternative ensemble model underperformed with respect to the chosen ensemble model.

$~$

## IX. Final Results - Validating the Final Model on the Validation Set

From the models trained on the training set, let's get probabilities of "deposit"
on the validation set for glm, rf and gbm. Then, an ensemble model will be built 
on the validation set just as it was on the test set, with a majority vote and 
a threshold of 0.086 obtained on the test set (see above).

```{r Running the ensemble model on the validation set.}
# Retrieving the fits obtained on the training set for glm, rf and gbm and the probability threshold optimized on the test set.
v <- c("glm", "rf", "gbm")
fits_3 <- fits[v]
threshold <- 0.086

# Getting probabilities on the validation set by using the fits obtained on the training set.
prob_val <- lapply(fits_3, function(object) 
  predict(object, newdata = bank_val, type = "prob"))
seq <- seq(1, (length(fits_3) * 2) - 1, 2)
prob_val <- prob_val %>% as.data.frame() %>% dselect(seq)
names(prob_val) <- v
rm(v, seq)

# Organizing majority vote.
val_ensemble <- data.frame(ens_s = 1, ens_r = 1, stringsAsFactors = FALSE)
dummy <- prob_val
dummy[dummy > threshold] <- 1
dummy[dummy <= threshold] <- 0
dummy <- dummy %>% as.data.frame()
votes <- rowSums(dummy)
votes <- ifelse(votes >= 2, "deposit", "no_deposit") %>% 
  factor(levels = levels(bank_val$y))

# Calculating numbers of subscribers and customers on validation set.
val_subs_number <- bank_val %>% filter(y == "deposit") %>% nrow() 
val_cust_number <- nrow(bank_val)

# Calculating sensitivity and precision for the ensemble model on the validation set.  
sensitivity <- confusionMatrix(votes, bank_val$y)$byClass[1]
TP <- round(val_subs_number * sensitivity) 
val_ensemble[1, 1] <-  sensitivity * 100
  
precision <- confusionMatrix(votes, bank_val$y)$byClass[3]
FP <- round(TP * (1 - precision) / precision)
val_ensemble[1, 2] <-  (val_cust_number - TP - FP) / val_cust_number * 100
```

Let's remember the objective, quantified and summarized in the objective table, which is reprinted here. 

$~$

```{r Reprinting objective table to evaluate final results.}
kable(objective_table, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16)
```

$~$

Let's analyse the final results on the validation set. The challenge was to meet one out the three alternatives from the table above, alternatives considered as equally valid.

$~$

```{r Printing the final results for the ensemble model on the validation set.}
results_tab <- val_ensemble %>% 
  mutate(ens_s = format(ens_s, digits = 0, nsmall = 0)) %>%
  mutate(ens_s = paste(ens_s, "%", sep = " ")) %>% 
  mutate(ens_r = format(ens_r, digits = 0, nsmall = 0)) %>%
  mutate(ens_r = paste(ens_r, "%", sep = " ")) %>%
  mutate(Model = "Ensemble glm-rf-gbm") %>% 
  mutate(Probability_Threshold = threshold) %>%
  rename(Subscribers_Reached = ens_s, Global_Coverage_Reduction = ens_r) %>% 
  dselect(Model, Probability_Threshold, Subscribers_Reached, Global_Coverage_Reduction) %>%   as.data.frame()
kable(results_tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>% 
  column_spec(1:4, width = "2in")
```

$~$

The final results fully meet all alternatives. They not only meet all alternatives but they do better than all alternatives. Indeed, in the final results, the percentage of the subscribers reached is 77, thus higher than the percentage of subscribers in any alternative. Moreover, in the final results, global coverage reduction is 52%, which is also higher than in any alternative. 

So, the final results not only meet the objective but do even better. 

In this scenario, by contacting only customers for whom the ensemble model has predicted "deposit", i.e. 48% (100 - 52) of the customers, the bank would reach 77% of the customers who have really subscribed term deposits.  

In a nutshell, 48% of customers contacted, 77% of subscribers reached. 

$~$

## X. Conclusion

At the end of this project, there are **meaningful results both from a marketing and from a data science point of view**. 

In direct bank marketing, it seems interesting:  **sampling less than 50% of a population of customers and still liaising with more than three-fourths of customers who are interested in the product you are trying to promote**. 

In data science, it has been an opportunity to get in touch with real data from the banking sector, which is by international standards a sector highly interested in data science. 

From an analytical point of view, exploratory analysis has delivered statements that are sometimes truisms but also sometimes surprising, which means that nothing in data should be taken for granted. 

**Cross-validation** by tuning the probability threshold has proved very powerful. 

**Using a test set**, even of limited size, showed itself decisive in reaching, and even exceeding the target. Running models on the test set has allowed to get rid of overfitting in the case of one model and to deselect a less performing model. Furthermore, it has allowed to build up an ensemble model, which has been very productive, just as cooperation in a team can make a difference.  

**Combining models demonstrated that, just as in the case of data, there can be surprises**: in a specific context, some computationally demanding models such as Random Forests can prove performing but it is also possible to get help from less demanding models such Logistic Regression, which in this project was individually less performing but nevertheless a decisive contributor to the ensemble model.

As a conclusion, this project has proved very stimulating and enriching and is a **powerful incentive towards business and data science**. 

## *****************************************************************************







