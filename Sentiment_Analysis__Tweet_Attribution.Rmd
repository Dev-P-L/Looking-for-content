---
title: "Tweet Attribution"
subtitle: "Trump's Tweets"
author: "Philippe Lambot"
date: "July 15, 2020"

output: 
  html_document:
    toc: true
    toc_depth: 4
    number_sections: true
    toc_float: 
      collapsed: false
      smooth_scroll: false
    highlight: espresso
    df_print: paged
    theme: readable
    code_folding: hide
---

```{r Setup}
# Avoiding messages and warnings: anyway, they have already been dealt with.  
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
# Regulating figure layout.
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
# Facilitating table layout in HTML.
options(knitr.table.format = "html")
# Output language
invisible(Sys.setlocale("LC_ALL", "C"))
```

<style type = "text/css">                  # Title colors
h1 {color: #08457E}
h2 {color: #08457E}
h3 {color: #08457E}
h4 {color: #08457E}
</style>

<style type = "text/css">                  # No bullets in TOC
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0;
}
</style>

<style type = "text/css">                  # Font size
<font size = "3">
</style>

<style type = "text/css">                  # Double text justification
body {
text-align: justify}
</style>

<br>

# EXECUTIVE SUMMARY

<br>

**Tweet attribution** is the topic of this project. Tweets come the account of Candidate Donald Trump during the 2016 presidential election campaign. Two devices have been used to issue tweets: and Android device and an iPhone. The challenge has been to predict the device on the validation set. 

Three types of predictors have been used:

- first, differences in **time patterns**, identified through **Exploratory Data Analysis**;
- second, differences in **token frequencies** (unigrams and bigrams), brought out through **Natural Language Processing** and **Text Mining**;
- third, differences in **hyperbolism**, quantified through **Sentiment Analysis**.

The existence of differences in hyperbolism had been clearly exposed by ... For the record, ... quantified the differences in hyperbolism; our results and conclucions partially diverge from his. During he 2016 US presidential election then candidate Donald J. Trump used his tweeter account as a way to communicate with potential voters. On August 6, 2016 Todd Vaziri tweeted about Trump that "Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him)." Data scientist David Robison conducted an analysis to determine if data supported this assertion. Here we go through David's analysis to learn some of the basics of text mining. 

Tweet attribution has been operated through Machine Learning with two algorithms: Support Vector Machine and eXtreme Gradient Boosting. Several models have been tried and their performances have been evaluated thanks to bootsrapped resampling. The performance metric has been accuracy because the proportion of tweets sent by each device is close to 50 %, which means that a baseline model would have accuracy performance hardly larger than 50 %. 

On the validation set, the accuracy metric has reached ... %, ... percentage points being brought by Sentiment Analysis, ... percentage points by token frequencies and ... percentage points by time predictors.

Beyond prediction performance, previous Sentiment Analysis had mentioned a larger probability ... This project shows that ... Moreover, results are diametrically different when expanding hashtags since most hashtags are in tweets issued by ... 

A clear caveat should be issued: these results are based on Sentiment Analysis based on feelings associated with words. This does not at all take into account context, e.g. sarcasm, which is outside the scope of this project. This does not take on board negation either although negation can reverse sentiment polarity. To investigate that avenue, some Text Analytics has been conducted, showing that negation ...

A second caveat should be clearly expressed about the scope of this project. Data originate from the R package *dslabs*, and in particular from the dataset *trump_tweets*...; the dataset *trump_tweets* has itself been built up out of ...; this means that usage of this project is limited to ... Moreover, this project is merely technical; it expresses absolutely no political vision or standpoint; it is in no way person-related; and the author's methods, results and conclusions are only the ones explicitely expressed in this project itself, which only encompasses files lodged with the GitHub repository ...  


TAGS: tweet attribution, time patterns, tokens, unigrams, bigrams, Natural Language Processing, Text Mining, Sentiment Analysis, lexicons, Bing, ncr, afinn, loughran, wordcloud2, comparison wordcloud, machine learning, Generalized Logistic Regression, Support Vector Machine, eXtreme Gradient Boosting, bootstrapped resampling, etc.

<br>

GITHUB: https://github.com/Dev-P-L/Sentiment_Analysis__Tweet_Attribution

<br>

# WELCOMING READERS

Dear Readers,


For your convenience, the final document, i.e. ..., is an HTML document with all code available on demand, by pushing tag buttons on the right-hand-side of the HTML document. 

Furthermore, for everyone's convenience, I have tried using color-blind-friendly colors, following pieces of advice given at 
http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/ .

Starting from their cbbPalette of eight colors, I have first of all picked up two colors to represent the two smartphones that issued tweets: both colors suffer no or almost no alteration neither in Protan nor in Deutan simulation and remain very distinct in Tritan simulation. These colors are "#E69F00" (orange for Android) and "#0072B2" (blue for iPhone). 

When used as background colors, orange and blue have each their subset of colors from ccbPalette. 

```{r Cleaning up workspace for RAM management}
invisible(if(!is.null(dev.list())) dev.off())
rm(list=ls())
cat("\014")
```

```{r Palettes}
# Picking up the main two colors: orange for Android and deep blue for iPhone. When used as background, they are used separately. 
orange <- "#E69F00"
deep_blue <- "#0072B2"

# When used as main colors on one figure, both colors are combined in a duo palette just with orange and deep blue, the colors assigned to respectively Android and iPhone. Let's use a named vector as recommended.
duo_Palette <- c(Android = "#E69F00", iPhone = "#0072B2")
# This palette will be associated with sky blue background:
sky_blue <- "#56b4e9"

# Palette for white background color: orange, deep blue, black, sky blue, bluish green, vermilion and reddish purple (yellow has been omitted)
cbf_Palette_w_b <- c("#E69F00", "#0072B2", "#000000", "#56b4e9",  
                "#009E73", "#D55E00", "#CC79A7")

# Palette for orange background color: tense blue, black, sky blue, bluish green, vermilion and reddish purple (yellow has been omitted ... as well as orange)
cbf_Palette_o_b <- c("#0072B2", "#000000", "#56b4e9",  
                     "#009E73", "#D55E00", "#CC79A7")

# Palette for deep blue background color: orange, sky blue, black, yellow, bluish green, vermilion and reddish purple
cbf_Palette_b_b <- c("#E69F00", "#56b4e9", "#000000", "#F0E442",  
                     "#009E73", "#D55E00", "#CC79A7")
```

For code, in the YAML, for highlight the value chosen is espresso, which, to my best knowledge, takes previous pieces of advice into account and, I hope, can be read with satisfaction by everyone. If code were not readily readable, you only have to change the value of *highlight* (please see available values at ... ) in the YALM and knit it again. 

You are most welcome to knit file ... to produce the document ... It only takes ... on my computer. For the record, here are some characteristics of my work environment: 

...

While knitting the file ..., I got a little bit into trouble with two tasks, and I was obviously not the only one according to complaints on the internet:

- downloading the lexicon ncr from package textdata 
- and with producing several wordcloud2 figures. 

Tips found on the internet are available here and here. These tips have worked perfectly well for me. Thanks you to people who helped on the internet. Here links to the internet. Great pieces of advice. 

They are repeated in comments on the code below and also further in this document when using these functions. 

<br>

# DOWNLOADING DATA & R PACKAGES

<br>

Besides R packages related to data, *tidyverse*, Machine Learning, wordclouds, etc., there are very different packages related to texts: *textreg*, *tm*, *quanteda*, *tidytext*, *stringr*, *textdata*, etc. 

```{r Cleaning up workspace and downloading packages}
# Package containing the dataset
if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
# Packages associated with tidyverse 
# Other packages could be added to this group but have been
# linked to text processing.
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
# Packages associated with R Markdown
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
# Packages related to NLP, Text Mining or Sentiment Analysis
# If you get into trouble while trying to access lexicon ncr
# from package textdata, I suggest having a look at 
# https://github.com/juliasilge/tidytext/issues/146 .
if(!require(utf8)) install.packages("utf8", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(tm)) install.packages("tm", repos = "http://cran.us.r-project.org")
if(!require(textreg)) install.packages("textreg", repos = "http://cran.us.r-project.org")
if(!require(quanteda)) install.packages("quanteda", repos = "http://cran.us.r-project.org")
if(!require(tidytext)) install.packages("tidytext", repos = "http://cran.us.r-project.org")
if(!require(textdata)) install.packages("textdata", repos = "http://cran.us.r-project.org")
# Packages associated with worclouds
# If you get into trouble with package and function wordcloud2,
# I suggest having a look at 
# https://github.com/Lchiffon/wordcloud2/issues/65 .
if(!require(wordcloud)) install.packages("wordcloud", repos = "http://cran.us.r-project.org")
if(!require(wordcloud2)) install.packages("wordcloud2", repos = "http://cran.us.r-project.org")
if(!require(devtools)) install.packages("devtools", repos = "http://cran.us.r-project.org")
if(!require(shiny)) install.packages("shiny", repos = "http://cran.us.r-project.org")
if(!require(httpuv)) install.packages("httpuv", repos = "http://cran.us.r-project.org")
if(!require(xtable)) install.packages("xtable", repos = "http://cran.us.r-project.org")
if(!require(sourcetools)) install.packages("sourcetools", repos = "http://cran.us.r-project.org")
if(!require(fastmap)) install.packages("fastmap", repos = "http://cran.us.r-project.org")
# Packages for Machine Learning
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(kernlab)) install.packages("kernlab", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
# Requiring libraries.
library(dslabs)
library(tidyverse)
library(scales)
library(lubridate)
library(ggthemes)
library(kableExtra)
library(gridExtra)
library(utf8)
library(stringr)
library(tm)
library(textreg)
library(quanteda)
library(tidytext)
library(textdata)
library(wordcloud)
library(wordcloud2)
library(devtools)
library(shiny)
library(httpuv)
library(xtable)
library(sourcetools)
library(fastmap)
library(caret)
library(kernlab)
library(xgboost)
library(pROC)
# Prevents silently failing after the first wordcloud2.
# Please see https://github.com/Lchiffon/wordcloud2/issues/65 .
devtools::install_github("gaospecial/wordcloud2")
```

Data are downloaded from the dataset *trump_tweets* from the R package *dslabs*. 

Let's normalize the dataset into utf8. Otherwise, a problem was previously encountered with apostrophes: the difference between curly and straight apostrophes can impede stopword removal, especially so in case of contractions; moreover, it can also bias token frequency counts by splitting appearances of the same token.

```{r Downloading data}
data("trump_tweets")
tweets <- trump_tweets %>% 
  mutate(text = sapply(text, utf8_normalize, map_quote = TRUE))
# Otherwise curly apostrophes remain.
```

Let's have a look at features from that dataset.

<br>

```{r Getting in touch with data}
str(tweets, vec.len = 1)
```

<br>


```{r Checking some features}
sum(tweets$is_retweet)
sum(is.na(tweets$in_reply_to_user_id_str))/nrow(tweets)
```

Documentation available at ?trump_tweets.

No retweet, so exlusion of feature. There can be some copy-paste. 

*in_reply_to_user_id_str* can be taken into account. 

The tweets are represented by the text variable.

The source variable tells us the device that was used to compose and upload each tweet.

Let's extract the relevant data, i.e. tweets issued by the Android device and by the iPhone. 

We are interested in what happened during the campaign, so for the analysis here we will focus on what was tweeted between the day Trump announced his campaign and election day. So we define the following table.

```{r Extracting relevant data}
buffer <- tweets %>% 
  mutate(device = str_replace_all(
    str_replace_all(source, "Twitter for Android", "Android"), 
    "Twitter for iPhone", "iPhone")) %>%
  filter(device %in% c("Android", "iPhone") &
         created_at >= ymd("2015-06-17") & 
         created_at < ymd("2016-11-08")) %>%
  select(- is_retweet, - source)
rm(tweets)
```

<br>

# SPLITTING DATASET INTO TRAINING SET AND VALIDATION SET

<br>

Let's take a prudent look at the number of rows and the breakdown between devices before splitting data in order to preempively avoid suboptimal representation. 

<br>

```{r Counting}
buffer %>% 
  group_by(device) %>% 
  summarise(n = n()) %>%
  arrange(desc(n))
```

<br>

The table above tells us, among others, that

- the total number of rows is 3,950,
- the breakdown between Android and iPhone is 53 % - 47 %.

The number of rows suffices to reach statistical representativeness even in case of splitting into training set and validation set. It would be unadvisable to split more finely between training set, test set and validation set, though: this would further reduce the size of samples 
and, anyway, training set model optimization can be readily operated through e.g. bootstrapped resampling. 

The splitting proportion will be two thirds for the training set and one third for the validation set. 

The almost fifty-fifty breakdown is compatible with our using accuracy as the performance metric since a baseline model would only predict device in 54 % of cases: this leaves leeway to better predict ... Anyway, to keep on the safe side, a more global performance metric will also be applied, i.e. the ROC AUC. 

```{r Splitting into training and validation sets}
set.seed(1)
ind_val <- createDataPartition(y = buffer$device, 
                               times = 1, p = 1/3, list = FALSE)
ind_train <- as.integer(setdiff(1:nrow(buffer), ind_val))
train_tweets <- buffer[ind_train, ]
val_tweets <- buffer[ind_val, ]
rm(buffer)
```

<br>

```{r Counting in training set}
train_tweets %>% 
  group_by(device) %>%
  summarise(n = n()) %>%
  arrange(desc(n))
```
 
<br>

Now, let's get insights from the four groups of predictors:

- publication time: *created_at*,
- interaction: *in_reply_to_user_id_str*, *retweet_count* and *favorite_count*,
- word frequencies in *text*,
- sentiment nature and intensity in *text*.

<br>

# EDA of TIME COMPONENTS

## Decomposing Datetime Information

<br>

For each tweet, we will extract from *created_at* 

- the month,
- the week, 
- the day,
- the hour (in the 24-hour clock, East Coast time zone (EST)),
- the day section (am or pm).

Extracting the 12 hour interval as am/pm values has been done using a piece of advice of Jaap's on Stack Overflow
https://stackoverflow.com/questions/37896824/grouping-time-and-counting-instances-by-12-hour-bins-in-r

```{r Extracting time components}
train_tweets <- train_tweets %>%
  mutate(month = floor_date(with_tz(created_at, "EST"), unit = "month")) %>%
  mutate(week = floor_date(with_tz(created_at, "EST"), unit = "week")) %>%
  mutate(day = floor_date(with_tz(created_at, "EST"), unit = "day")) %>%
  mutate(hour = hour(with_tz(created_at, "EST"))) %>%
  mutate(am_pm = gsub('[0-9: ]+', '\\1', format(created_at, '%r'))) %>%
  arrange(created_at) %>%
  select(device, everything()) 
```

Let's investigate tweet concentration per hour in the 24-hour clock.

<br>

```{r Graph of activity per hour and per device}
# Preparing tick values and labels for x axis.
seq_ticks <- seq(min(train_tweets$hour), max(train_tweets$hour), 4)

graph <- train_tweets %>%
  select(device, hour) %>%
  group_by(device, hour) %>%
  summarize(n = n()) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(hour, percent, color = device)) +
  geom_line(size = 3) +
  scale_x_continuous(breaks = seq_ticks, labels = waiver()) +
  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +
  ggtitle("Tweet Activity per Hour per Device") +
  labs(x = "Hour in the 24-Hour Clock",
       y = "% of Tweets per Hour",
       color = "Device") +
  theme(plot.title = element_text(hjust = 0.5, vjust = 3, 
                                  size = 16, face = "bold"),
        axis.title.x = element_text(vjust = -1, size = 14), 
        axis.title.y = element_text(vjust = 3, size = 14), 
        legend.title = element_text(size = 14),
        axis.text.x = element_text(hjust = 1, size = 12), 
        axis.text.y = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  scale_color_manual(values = duo_Palette) +
  theme(panel.background = element_rect(fill = sky_blue)) 
graph
```

We notice a big peak for the Android in early hours of the morning, between 6 and 8 AM. There seems to be a clear different in these patterns. We will therefore assume that two different entities are using these two devices. 

It is like adding apples and pears: day schedules are amalgamated over more than one year. Would find back that pattern sytematically in individual day schedules? Let's check up this assumption without amalgamating all days, which can be misleading. 

The objective reads as follows:

- will most day schedules have Android activity in the morning and iPhone activity only pm?
- or will most day schedules have Android activity at least mostly concentrated in the morning and iPhone activity mostly concentrated in pm?
- will most day schedules show no temporal intersection?

Samples of day schedules have been picked up:

- 12 day schedules taken at random,
- 12 day schedules corresponding to the busiest days in global tweet activity,
- 12 day schedules corresponding to the busiest days in Android tweet activity,
- 12 day schedules corresponding to the busiest days in iPhone tweet activity.

Results will be shown under the form of graphs.

```{r Developping function to produce the 10 graphs}
# Scale of x tick marks to be incorporated into ggplot2 graphs
seq_ticks_x <- seq(min(train_tweets$hour), max(train_tweets$hour), 4)

# Creating a function to produce series of day schedule graphs.

building_graph_series <-                            # Function name
  function(d, sub_title) {                          # d = sample of days
    
    tweets_per_day <- train_tweets %>%              # Filtering data
     select(device, day, hour) %>%                  # based on d 
     group_by(device, day, hour) %>%              
     filter(day %in% d) %>%
     summarize(n = n()) %>%
     as.data.frame()
    
    l <- list(1)                                    # Storage list
                                                    # for graphs
    
    for (i in seq_along(d)) {                       # For loop on 
                                                    # sample of days d
      
      buffer <- tweets_per_day %>%                  # Further filtering
        filter(day == d[i]) %>%                     # data for day i.
        as.data.frame()
      
      seq_ticks_y <-                                # y tick marks
        seq(min(buffer$n), max(buffer$n), 1)
                                                    # Graph for day i
      graph <- buffer %>%                                                          ggplot(aes(hour, n, color = device)) +      
        geom_point(aes(size = n)) +                 # Gradience of dots
        scale_size_continuous(range = c(4, 6)) +    # Size of dots
        guides(size = FALSE) +                      # No legend for size
        scale_x_continuous(breaks = seq_ticks_x, labels = waiver()) +
        scale_y_continuous(breaks = seq_ticks_y, labels = waiver()) +
        labs(title = paste("Day", i, ":", d[i], sep = " "),
             subtitle = sub_title, 
             x = "Hour in the 24-Hour Clock",
             y = "# of Tweets per Hour",
             color = "Device") +
        theme(plot.title = element_text(hjust = 0.5, vjust = 2, 
                                        size = 16, face = "bold"),
              plot.subtitle = element_text(hjust = 0.5, vjust = 3, 
                                           size = 12),
              axis.title.x = element_text(vjust = -1, size = 14), 
              axis.title.y = element_text(vjust = 3, size = 14), 
              legend.title = element_text(size = 14),
              axis.text.x = element_text(hjust = 1, size = 12), 
              axis.text.y = element_text(size = 12),
              legend.text = element_text(size = 12)) +
              scale_color_manual(values = duo_Palette) +
              theme(panel.background = element_rect(fill = sky_blue)) 
  
    l[[i]] <- graph                                 # Placing output
    }                                               # into list.
    
marrangeGrob(l, nrow = 1, ncol = 1, top = "")       # Printing graphs.
}
```

The corresponding 48 day schedule graphs have been produced and analyzed. For illustrative purposes, some of them are reproduced below: 

- 4 graphs picked up at random,
- 2 graphs with the top days in global activity,
- 2 graphs with the top days in Android activity,
- 2 graphs with the top days in iPhone activity.

Conclusions drawn on the basis of the 48 graphs or on the basis of the 10 graphs are the same. Let's examine the 10 graphs, group by group. Let's get started with the 4 graphs produced at random. 

<br>

```{r 4 graphs at random}
# Let's pick up row numbers at random.
sample_size <- 4
days <- unique(train_tweets$day)
set.seed(1)
sample_r <- sample(days, sample_size, replace = FALSE)
# Let's store day identification in the vector sample_r ("r" for random).

sub_title <- "Selection Method: Random Pick"

building_graph_series(sample_r, sub_title)
```

<br>

In day 1, Android activity spread in the morning and in the evening. 

In day 2, as described am-pm.

On day 3, Android activity spread in the morning and in the evening.

On day 4, Android activity spread in the morning and in the evening, with even more in the evening; moreover, concomittance Android and iPhone!

Let's turn to the top activy days. 

<br>

```{r 2 graphs of top actitiy days}
# Let's build up a sample based on global tweet activity per day,
# called sample_g, g being for "global tweet activity".
sample_size <- 2
sample_g <- train_tweets %>%
  select(day) %>% 
  filter(!day %in% sample_r) %>%
  group_by(day) %>% 
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  head(., sample_size) 

# Keeping the 2 top days for printed graphs.  
d <- sample_g$day

sub_title <- "Selection: Days with Busiest Global Tweet Activity"

building_graph_series(d, sub_title)
```

<br>

<br>

```{r 2 graphs of top Android actitiy days}
# Vector of days
days <- unique(train_tweets$day)

# Excluding the 2 day vectors previously used.


# Let's build up a sample based on global tweet activity per day,
# called sample_g, a being for "Android tweet activity".
sample_size <- 2
sample_a <- train_tweets %>%
  filter(device == "Android") %>%
  select(day) %>%  
  filter(!day %in% sample_r & 
           !day %in% sample_g$day) %>%
  group_by(day) %>% 
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  head(., sample_size) 

# Keeping the 2 top days for printed graphs.  
d <- sample_a$day

sub_title <- "Selection: Days with Busiest Android Tweet Activity"

building_graph_series(d, sub_title)
```

<br>

<br>

```{r 2 graphs of top iPhone actitiy days}
# Vector of days
days <- unique(train_tweets$day)

# Let's build up a sample based on global tweet activity per day,
# called sample_i, i being for "iPhone tweet activity".
sample_size <- 2
sample_i <- train_tweets %>%
  filter(device == "iPhone") %>%
  select(day) %>%  
  filter(!day %in% sample_r & 
           !day %in% sample_g$day &
           !day %in% sample_a$day) %>%
  group_by(day) %>% 
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  head(., sample_size) 

# Keeping the 2 top days for printed graphs.  
d <- sample_i$day

sub_title <- "Selection: Days with Busiest Android Tweet Activity"

building_graph_series(d, sub_title)
```

<br>

Visual evidence provided by the 10 day schedule graphs cannot support previous opinion about two separate entities tweeting; it does not infirm that hypothesis either. 

Let's get broader insights about dayly time schedule, using the whole dataset. 

Hour slots

```{r Bi-device time slots}
# Let's check whether created_at is strictly increasing
all(diff(train_tweets$created_at) > 0)
all(diff(as.numeric(train_tweets$id_str)) > 0)

# Let's build up a temporary data frame with Android only
df_a <- train_tweets %>%
  mutate(row = as.numeric(rownames(train_tweets))) %>%
  filter(device == "Android") %>%
  select()

str(rownames(train_tweets))

difftime(v[3], v[1], units = "hours")
```

<br>

...

<br>

```{r Counting tweets in time slots in which both devices produced tweets}

```


## Month Evolution

All datetime values have been aggregated by month beginning. This gives chronological evolution with a strong smoothing effect. This month component will be used in this section to show activity per month and per device. Since there are more iPhone tweets than Android tweets, for comparability reasons, activity per month is expressed as an activity percentage per device.



```{r Graph of activity per month and per device}
graph <- train_tweets %>%
  select(device, month) %>%
  group_by(device, month) %>%
  summarize(n = n()) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(month, percent, color = device)) +
  geom_line(size = 3) +
  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +
  ggtitle("Tweet Activity per Month per Device") +
  labs(y = "% of Tweets per Month",
       color = "Device") +
  theme(plot.title = element_text(hjust = 0.5, vjust = 3, 
                                  size = 16, face = "bold"),
        axis.title.x = element_blank(), 
        axis.title.y = element_text(vjust = 3, size = 14), 
        legend.title = element_text(size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12), 
        axis.text.y = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  scale_color_manual(values = duo_Palette) +
  theme(panel.background = element_rect(fill = sky_blue)) 
graph
```

<br>

Interpreting curve evolution would probably require additional information; it is out of the scope of this project. 

General tendency is rather similar up to July 2016; afterwards, Android activity plummets while Iphone activity soars. 

Moreover, in levels, at the beginning of the period, the activity percentage of the Android device is higher than the one from the iPhone. 

These differences qualify this feature as a candidate predictor in tweet attribution. 







---------------------------------------------------------------------------------

EVOLUTION OF TWEET PUBLICATION

```{r Graph Tweet publication count}
graph <- elect_tweets %>%
  mutate(week = round_date(with_tz(created_at, "EST"), unit = "week")) %>%
  select(device, week) %>%
  group_by(device, week) %>%
  summarize(n = n()) %>%
  ggplot(aes(week, n, color = device)) +
  geom_line(size = 2) +
  labs(x = "",
       y = "% of Tweets per Device",
       color = "") +
  theme_economist(base_size = 12) +
  scale_colour_manual(values = cbbPalette) +
  theme(panel.background = element_rect(fill = "#56B4E9")) 
graph
```

-------------------------------------------------------------------

```{r Graph retweet_count}
graph <- elect_tweets %>%
  mutate(month = floor_date(with_tz(created_at, "EST"), unit = "month")) %>%
  select(device, month, retweet_count) %>%
  group_by(device, month) %>%
  summarize(avg = mean(n())) %>%
  ggplot(aes(month, avg, color = device)) +
    geom_line(size = 2) +
  labs(x = "",
       y = "% of Tweets per Device",
       fill = "Parts of the Day") +
  theme_economist(base_size = 12) +
  scale_color_manual(values = cbbPalette) +
  theme(panel.background = element_rect(fill = "#56B4E9")) 
graph
```


```{r Graph am_pm 24 hour-clock}
graph <- elect_tweets %>%
  mutate(hour = hour(with_tz(created_at, "EST"))) %>%
  select(device, am_pm) %>%
  group_by(device, am_pm) %>%
  summarise(n = n()) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(device, percent, fill = am_pm)) +
  geom_bar(width = 0.5, stat = "identity") +
  scale_y_continuous(labels = percent_format(suffix = " %")) +
  labs(x = "",
       y = "% of Tweets per Device",
       fill = "Parts of the Day") +
  theme_economist(base_size = 12) +
  scale_fill_manual(values = cbbPalette) +
  theme(panel.background = element_rect(fill = "#56B4E9"))
graph
```

Now in hour.

```{r Graph am_pm 24 hour-clock}
# The palette with grey:
# Orange, blue, black, light blue, green, yellow, vermilion, reddish purple
cbbPalette <- c("#E69F00", "#0072B2", "#000000", "#56B4E9", 
                "#009E73", "#F0E442",  "#D55E00", "#CC79A7")
graph <- elect_tweets %>%
  mutate(hour = hour(with_tz(created_at, "EST"))) %>%
  select(device, hour) %>%
  group_by(device, hour) %>%
  summarise(n = n()) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(hour, percent, color = device)) +
  geom_point(aes(size = percent)) +
  scale_y_continuous(labels = 
                     percent_format(accuracy = 1, suffix = " %")) +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "") +
  theme_economist(base_size = 12) +
  scale_colour_manual(values = cbbPalette) +
  theme(panel.background = element_rect(fill = "#56B4E9"))
graph
```



```{r Adapting opts_chunk to enlarge display width for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

percentage <- 
  label_percent(accuracy = 0.1, suffix = " %")(sum(sample$n) / nrow(buffer))

```{r Graph % AM per month for iPhone}
graph <- train_tweets %>%
  mutate(month = round_date(with_tz(created_at, "EST"), unit = "month")) %>%
  select(device, month, am_pm) %>%
  filter(device == "iPhone") %>%
  group_by(month, am_pm) %>%
  summarize(n = n()) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(month, percent, fill = am_pm)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = percent_format(suffix = " %")) +
  labs(x = "",
       y = "% of Tweets per Device",
       fill = "Parts of the Day") +
  theme_economist(base_size = 12) +
  scale_fill_manual(values = cbf_Palette_b_b) +
  theme(panel.background = element_rect(fill = "#56B4E9")) 
graph
```

Provisional conclusion 
######################

Activity by the two devices is intermingled in the sense that there is no clear-cut separation; the busiest hour periods differ clearly on average, but on a daily basis the busiest hour periods do not systematically differ. 

So, there are partial time leads and lags between devices. Would that be compatible with the hypothesis of two separate entities each using  one device? Actually, beyond the factual statement of these leads and lags, anything else is sheer speculation on the basis of information available. Moreover, the approach in this project is not at all person-related. Whether the two devices are or are not operated by the same entity or by two different entities is not material to our purposes. 

The factual statement of leads and lags is used in this EDA section to characterize tweets from the two devices. In machine learning, this piece of information can be used to attribute tweets to one particular device. 

-----------------------------------------------------------------------------


Now we will study how their tweets differ. 

NLP 

Text as data

The tidytext package helps us convert from text into a tidy table. Having the data in this format greatly facilitates data visualization and applying statistical techniques.

The main function needed to achieve this is unnest_tokens. A token refers to the units that we are considering to be a data point. The most common tokens will be words, but they can also be single characters, ngrams, sentences, lines or patterns defined by a regex. The functions will take a vector of strings and extract the tokens so that each one gets a row in the new table. 

```{r Exercise}
v <- data.frame(text = elect_tweets$text) 
v %>% unnest_tokens(word, text)
pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
elect_tweets[i,] %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  select(word)
```

Note that the function tries to convert tokens into words and strips characters important to twitter such as # and @. A token in twitter is not the same as in regular english. For this reason instead of using the default, words, we define a regex that captures twitter character. The pattern appears complex but all we are defining is a patter that starts with @, # or neither and is followed by any combination of letter or digits:

pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"

We can now use the unnest_tokens function with the regex option and appropriately extract the hashtags and mentions:

campaign_tweets[i,] %>% 
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  select(word)

Another minor adjustment we want to make is remove the links to pictures:

campaign_tweets[i,] %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  select(word)

Now we are now read to extract the words for all our tweets.

tweet_words <- campaign_tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) 

And we can now answer questions such as "what are the most commonly used words?"

tweet_words %>% 
  count(word) %>%
  arrange(desc(n))

It is not surprising that these are the top words. The top words are not informative. The tidytext package has database of these commonly used words, referred to as stop words, in text mining:

stop_words

If we filter out rows representing stop words with filter(!word %in% stop_words$word)

tweet_words <- campaign_tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  filter(!word %in% stop_words$word ) 

we end up with a much more informative set of top 10 tweeted words

tweet_words %>% 
  count(word) %>%
  top_n(10, n) %>%
  mutate(word = reorder(word, n)) %>%
  arrange(desc(n))

Some exploration of the resulting words (not show here) reveals a couple of unwanted characteristics in our tokens. First, some of our tokens are just numbers (years for example). We want to remove these and we can find them using the regex ^\d+$. Second, some of our tokens come from a quote and they start with '. We want to remove the ' when it's at the start of a word so we will use str_replace. We add these two lines to the code above to generate are final table:

tweet_words <- campaign_tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  filter(!word %in% stop_words$word &
           !str_detect(word, "^\\d+$")) %>%
  mutate(word = str_replace(word, "^'", ""))

Now that we have all our words in a table, along with information about what device was used to compose the tweet they came from, we can start exploring which words are more common when comparing Android to iPhone.

For each word we want to know if it is more likely to come from an Android tweet or an iPhone tweet. We previously introduced the odds ratio a summary statistic useful for quantifying these differences. We each device and a given word, let's call it y, we compute the odds or the ratio between the proportion of words that are y and not y and compute the ratio of those odds. Here we will have many proportions that are 0 so we use the 0.5 correction.

android_iphone_or <- tweet_words %>%
  count(word, source) %>%
  spread(source, n, fill = 0) %>%
  mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) / 
           ( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
android_iphone_or %>% arrange(desc(or))
android_iphone_or %>% arrange(or)

Given that several of these words are overall low frequency words we can impose a filter based on the total frequency like this:

android_iphone_or %>% filter(Android+iPhone > 100) %>%
  arrange(desc(or))

android_iphone_or %>% filter(Android+iPhone > 100) %>%
  arrange(or)

We already see somewhat of a pattern in the types of words that are being tweeted more in one device versus the other. However, we are not interested in specific words but rather in the tone. Vaziri's assertion is that the Android tweets are more hyperbolic. So how can we check this with data? Hyperbolic is a hard sentiment to extract from words as it relies on interpreting phrases. However, words can be associated to more basic sentiment such as as anger, fear, joy and surprise. In the next section we demonstrate basic sentiment analysis.

Sentiment Analysis

In sentiment analysis we assign a word to one or more "sentiment". Although this approach will miss context dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.

The first step in sentiment analysis is to assign a sentiment to each word. The tidytext package includes several maps or lexicons in the object sentiments:

table(sentiments$lexicon)

The bing lexicon divides words into positive and negative. We can see this using the tidytext function get_sentiments:

get_sentiments("bing")

The AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive.

get_sentiments("afinn")

The loughran and nrc lexicons provide several different sentiments:

get_sentiments("loughran") %>% count(sentiment)
get_sentiments("nrc") %>% count(sentiment)

To start learning about how these lexicons were developed read this help
file ?sentiments.

For the analysis here we are interested in exploring the different sentiments of each tweet so we will use the nrc lexicon:

nrc <- sentiments %>%
  filter(lexicon == "nrc") %>%
  select(word, sentiment)

We can combine the words and sentiments using inner_join, which will only keep words associated with a sentiment. Here are 10 random words extracted from the tweets:

tweet_words %>% inner_join(nrc, by = "word") %>% 
  select(source, word, sentiment) %>% sample_n(10)

Now we are ready to perform a quantitative analysis comparing Android and iPhone by comparing the sentiments of the tweets posted from each device. Here we could perform a tweet by tweet analysis, assigning a sentiment to each tweet. However, this somewhat complex since each tweet will have several sentiments attached to it, one for each word appearing in the lexicon. For illustrative purposes we will perform a much simpler analysis: we will count and compare the frequencies of each sentiment appears for each device.

sentiment_counts <- tweet_words %>%
  left_join(nrc, by = "word") %>%
  count(source, sentiment) %>%
  spread(source, n) %>%
  mutate(sentiment = replace_na(sentiment, replace = "none"))
sentiment_counts

Because more words were used on the Android than on the phone:

tweet_words %>% group_by(source) %>% summarize(n = n())

For each sentiment we can compute the odds of being in the device: proportion of words with sentiment versus proportion of words without and then compute the odds ratio comparing the two devices

sentiment_counts %>%
  mutate(Android = Android / (sum(Android) - Android) , 
         iPhone = iPhone / (sum(iPhone) - iPhone), 
         or = Android/iPhone) %>%
  arrange(desc(or))

So we do see some difference and the order is interesting: the largest three sentiments are disgust, anger, and negative! But are they statistically significant? How does this compare if we are just assigning sentiments at random?

To answer that question we can compute, for each sentiment, an odds ratio and confidence interval. We will add the two values we need to form a two-by-two table and the odd rat

library(broom)
log_or <- sentiment_counts %>%
  mutate( log_or = log( (Android / (sum(Android) - Android)) / (iPhone / (sum(iPhone) - iPhone))),
          se = sqrt( 1/Android + 1/(sum(Android) - Android) + 1/iPhone + 1/(sum(iPhone) - iPhone)),
          conf.low = log_or - qnorm(0.975)*se,
          conf.high = log_or + qnorm(0.975)*se) %>%
  arrange(desc(log_or))
  
log_or
A graphical visualization shows some sentiments that are clearly overrepresented:
log_or %>%
  mutate(sentiment = reorder(sentiment, log_or),) %>%
  ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point(aes(sentiment, log_or)) +
  ylab("Log odds ratio for association between Android and sentiment") +
  coord_flip() 
We see that the disgust, anger, negative sadness and fear sentiments are associated with the Android in a way that is hard to explain by chance alone. Words not associated to a sentiment were strongly associated with the iPhone source, whic is in agreement with the original claim about hyperbolic tweets.
If we are interested in exploring which specific words are driving these differences, we can back to our android_iphone_or object:
android_iphone_or %>% inner_join(nrc) %>%
  filter(sentiment == "disgust" & Android + iPhone > 10) %>%
  arrange(desc(or))
We can make a graph
android_iphone_or %>% inner_join(nrc, by = "word") %>%
  mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
  mutate(log_or = log(or)) %>%
  filter(Android + iPhone > 10 & abs(log_or)>1) %>%
  mutate(word = reorder(word, log_or)) %>%
  ggplot(aes(word, log_or, fill = log_or < 0)) +
  facet_wrap(~sentiment, scales = "free_x", nrow = 2) + 
  geom_bar(stat="identity", show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
------------------------------------------------------------------

COLORS

The first two references deal with color-blind-friendly issues. 

http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/

https://venngage.com/blog/color-blind-friendly-palette/#2

http://www.sthda.com/english/wiki/ggplot2-themes-and-background-colors-the-3-elements

https://ggplot2.tidyverse.org/reference/scale_manual.html
it's recommended to use a named vector

---------------------------------------------------------------------
tidytext package
https://www.tidytextmining.com/
janeaustenr package !!!!
https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html
REGEX
https://bookdown.org/rdpeng/rprogdatascience/regular-expressions.html
Removing unwanted characters from a corpus with tm package
https://community.rstudio.com/t/tm-package-removing-unwanted-characters-works-in-r-but-not-knitr/26734
Extracting the 12 hour interval as am/pm values has been done using a piece of advice of Jaap's on Stack Overflow
https://stackoverflow.com/questions/37896824/grouping-time-and-counting-instances-by-12-hour-bins-in-r

NLP
https://cran.r-project.org/web/views/NaturalLanguageProcessing.html
https://stackoverflow.com/questions/21533899/in-r-use-gsub-to-remove-all-punctuation-except-period/39745610
The \\1 is syntax for the last capture in a regular expression using the () 
It says whatever was matched, replace it with that. 
I put only the "." and "-" in the group (), so \\1 will replace .- 
(by the same vale), so it keeps them here. – agstudy Feb 3 '14 at 20:02

Extracting hashtags
https://stackoverflow.com/questions/13762868/how-do-i-extract-hashtags-from-tweets-in-r
There are two levels of parsing going on here. 
Before the low level regexp function within str_extract 
gets the pattern you want to search for (i.e. "#\S+") 
it is first parsed by R. R does not recognize \S 
as a valid escape character and throws an error. 
By escaping the slash with \\ you tell R to pass the \ and S 
as two normal characters to the regexp function, 
instead of interpreting it as one escape character.
v <- str_extract_all(train_tweets$text, "#\\S+")

SPLITTING BETWEEN LOWER CASE AND UPPER CASE

https://stackoverflow.com/questions/43706474/splitting-string-between-capital-and-lowercase-character-in-r
We can use regex lookaround to match lower case letters 
(positive lookbehind - (?<=[a-z])) followed by upper case letters 
(positive lookahead -(?=[A-Z]))

Not used but ...
https://cran.r-project.org/web/packages/textclean/textclean.pdf

---
https://blog.datazar.com/first-debate-2016-sentimental-analysis-of-candidates-58d87092fc6a
twitter authentication
web scraping
sentiment analysis
simplistic in comments?

https://www.kaggle.com/erikbruin/text-mining-the-clinton-and-trump-election-tweets
awesome!

LIMITS OF LEXICONS
https://hoyeolkim.wordpress.com/2018/02/25/the-limits-of-the-bing-afinn-and-nrc-lexicons-with-the-tidytext-package-in-r/

Pas moyen d'ouvrir nrc
https://github.com/juliasilge/tidytext/issues/146

Vocabulary

https://www.vocabulary.com/dictionary/wrath

https://www.fluentu.com/blog/english/american-english-slang-words-esl/
This one refers to other lists. 

President Trum linguistically is unadorned, oddly adolescent.
https://www.youtube.com/watch?v=phsU1vVHOQI
Professor John McWhorter
He uses tags at the end of a sentence: "believe me"
and enforcers 
more than ever before
more than we've seen before

Informal English
https://www.engvid.com/english-resource/formal-informal-english/

Teen slang
https://www.verywellfamily.com/a-teen-slang-dictionary-2610994

---------------------------------------------------------------------

STUDY ITSELF - BOOK FROM RAF
https://books.google.be/books?id=62K-DwAAQBAJ&pg=PA459&lpg=PA459&dq=str_replace_all(text,+https://t.co/%5BA-Za-z%5C%5Cd%5D%2B%7C%26amp;,+)&source=bl&ots=bDWhUw2slL&sig=ACfU3U3Kvhh-parwC0mYLod1HL_yvlWLnQ&hl=en&sa=X&ved=2ahUKEwiGycS7pNHpAhXElqQKHR7RDvcQ6AEwAHoECAoQAQ#v=onepage&q=str_replace_all(text%2C%20https%3A%2F%2Ft.co%2F%5BA-Za-z%5C%5Cd%5D%2B%7C%26amp%3B%2C%20)&f=false

Todd Vaziri
https://twitter.com/tvaziri/status/762005541388378112

David Robinson
http://varianceexplained.org/r/trump-tweets/
---------------------------------------------------------------------

WORDCLOUDS

Retrieving, unnesting and building up wordclouds
https://www.littlemissdata.com/blog/wordclouds

---

Focused on wordclouds, with Twitter logo!
https://cran.r-project.org/web/packages/wordcloud2/vignettes/wordcloud.html

---

Focused on wordclouds
https://www.r-graph-gallery.com/196-the-wordcloud2-library.html

---

Wordclouds and DRACULA gutenbergr
https://www.learningrfordatascience.com/post/dynamic-wordclouds-with-wordcloud2/

---------------------------------------------------------------------------

TICKS ON A DISCRETE NUMERIC X AXIS

https://stackoverflow.com/questions/47794265/changing-x-axis-ticks-in-ggplot2

ID is a numeric column, so ggplot2 uses a continuous scale, not a discrete scale:

---------------------------------------------------------------------------

GGPLOT2 SUBTITLES

hrbrmstr
https://stackoverflow.com/questions/11724311/how-to-add-a-ggplot2-subtitle-with-different-size-and-colour

---------------------------------------------------------------------------

GGPLOT2 SIZE OF DOT SYMBOLS
ialm

https://stackoverflow.com/questions/20251119/increase-the-size-of-variable-size-points-in-ggplot2-scatter-plot

---------------------------------------------------------------------------

GGPLOT2 SUPPRESSING LEGEND RELATED TO geom_point(aes(size = n))
Brandon Bertelsen
https://stackoverflow.com/questions/4207518/how-can-i-hide-the-part-of-the-legend-using-ggplot2

Actually didn't work because size grading disappeated with legend.

But
Didzis Elferts
https://stackoverflow.com/questions/14604435/turning-off-some-legends-in-a-ggplot

https://ggplot2.tidyverse.org/reference/scale_manual.html
it's recommended to use a named vector
---------------------------------------------------------------------------

https://stackoverflow.com/questions/12977073/how-to-find-the-difference-between-two-dates-in-hours-in-r
How to find the difference between two dates in hours in R?
Thank you Andrie

---------------------------------------------------------------------------